{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semester Project:\n",
    "# exploring Differentiable Neural Computers and applications\n",
    "\n",
    "## Introduction\n",
    "What we want to present in the following notebook is a cutting-edge topic in machine learning, related to a particular model of Neural Network called **Differentable Neural Computer** (DNC) which puts together a Neural Network and an external memory from which the Netwoek can read and write. The main source of this noteboook is based on the article and paper which can be found [here](https://deepmind.com/blog/differentiable-neural-computers/).\n",
    "<br>\n",
    "\n",
    "We start with a presentation of the DNC and how they are enhanced form *Neural Turing Machines*. Then we move to the core topic of this networks which are the three **differentiable attention mechanisms** with whihc the network performs the read and writes operations. We propose then a code for the implementation of these Networks, testing their performances and limitations. Finally we will try to implement the graph task presented in the paper and adapt it to a different problem.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNC\n",
    "The Differentiable Neural Computers are neural networks with a coupled memory, where the dimension of this memory does not affect the behaviour of the network. The memory can be tought as a RAM and the Network as a controller which is differentiable using gradient descent-like tecniques.\n",
    "<br>\n",
    "Comparing this Network with the respect of other whihc present memory states such as RNN or LSTM, DNCs do not store information whihc is tigth coupled with the model itself. Instead, they have the possibility to selectively read and write to and from memory locations, creating a separation between state preservation and content (data).\n",
    "<br>\n",
    "\n",
    "In the litterature, another network capable of read and write using an external memory came out in the past, called **Neural Turing Machine** (NTM). Altought the concept is similar to the DNC, with a Network acting as controller and an external memory, they are more restricted in the method with which the Neural Network access the memory. \n",
    "\n",
    "<br>\n",
    "![DNC](./figures/DNC_architecture.png)\n",
    "<br>\n",
    "\n",
    "The image shows the architecture of a DNC Neural Network. In this specif example, the controller is a Recursive Neural Network (a LSTM is also possible) which is receiving an input (from a dataset) and the previous step readings from the memory. The output of the network is made of two parts:\n",
    "- The output of the Neural Network, which constitute our target\n",
    "- A vector called **'Interface Vector'** which is used by the read and write heads to interact with the memory\n",
    "\n",
    "As it's clear by now, the DNC is mainly composed of two parts interacting together: the **Neural Netwoerk** itself (**a**) and the **Heads** (**b**) which performs the operations over the **Memory** (**c**). Those oprations are the key differences of DNC over NTM and will be described in details in the next paragraph. \n",
    "\n",
    "The Heads are particular components which are dealing with the content of the memory. It's possible to define as many read and write heads as needed, all of them will receive weights vector which are used to define the location over which perform the read and write operations. The differentiability of these parts allows the network to learn how to perform these operations simple by looking at the error during the trainign process and adjusting the weights.\n",
    "\n",
    "Finally, in addition to the Memory there is an additional information whcih is saved and used during the read and writes operations. Those are the **Memory Usage and Temporal Links** (**d**) which is an imprvement over the NTM. Those links and usage information allows to dynamically allocate the content during the reads and to have notion of temporal assosciation between entries of the memory. That is, it's possibile to know the sequence of the writtings which is extremely important when the Network has to deal to sequential tasks such as graph path search.\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Interactions\n",
    "\n",
    "### Overview\n",
    "The core mechanism of DNCs is the possibility to write and read from an external memory matrix. As mentioned, the difference with the NTMs is in defining more **attention mechanisms**. This is different from the address mechanisms in conventional computers, where there is a mapping between the address and the content of the memory. The idea here is to define the weights over the location of the Memory. Those weights represents a degree (we can immagine it has a filter) that indicates how much the locations in the Memory matrix are involved during read and writing operations.\n",
    "<br>\n",
    "Briefly:\n",
    "- a read vector $\\mathbf{r}$ is returned after a read operation which involves a read weights matrix $\\mathbf{w^r}$:<br>\n",
    "    $\\mathbf{r}=\\sum_{i=1}^N M[i,j]w^r[i]$ for $j=1, \\dots,W$\n",
    "    \n",
    "- an erase vector $\\mathbf{e}$ is applied using the write weights matrix $\\mathbf{w^w}$, then a a write vector is added $\\mathbf{v}$:<br>\n",
    "    $\\mathbf{v}: M[i,j] \\leftarrow M[i,j](1-\\mathbf{w^w}[i]\\mathbf{e}[j]+\\mathbf{w^w}[i]\\mathbf{v}[j])$\n",
    "\n",
    "The units which determines this operations are the read and write Heads. \n",
    "\n",
    "### Differentiable Attention\n",
    "There are three forms of differentiable attention. This is more a fancy terminology to call the three pricipal techniques involved in in order to address the memory. Before going into the details we want to recall some of the techniques which are instead used in NTM to appreasciate the difference with the DNC. \n",
    "\n",
    "There are two mechanism for addressing the memory and the Neural Tuning Machines combines both:\n",
    "- *content-based addressing*: focuses attention on locations based on the similarity between their current values and values emitted by the controller. This is related to the content addressing of the Hopfield networks, the controller needs to generate a value which is an approximation of the one stored to then retrieve the exact location. \n",
    "- *location-based addressing*: it's the traditional approach to address the memory. For arithmetic operation where we need to define variables the conent based in not enough, we need the location of the variable to perform the operation.\n",
    "\n",
    "Differentiable Neural Computers uses a *content-based addressing* paired with other two techniques which allows for a **Dynamic Memory Addressing** counteracting the major drawbacks of the NTM:\n",
    "1. NTMs do not avoid possible overlapping and interfere among blocks of allocating memory. DNCs instead overcame this problem due to there is only a single free at each write time.\n",
    "2. NTMs does not allow for freeing location of memory which are not used and this can be a problem when processing long sequences. DNCs instead can free memory locations based on the usage weights.\n",
    "3. NTMs sequential information is preserved only if the content is written in consecutive locations. DNCs uses an additional temporal link matrix avoidin the restriction to continuous locations only. \n",
    "\n",
    "For now we explain the concepts of these three differentiable attention. A more detailed analysis will be given in the following paragraphs\n",
    "\n",
    "#### Content base addressing\n",
    "This form is used to determine the similarity measure between the vector emmited by the controller and the content of the memory. The measure is a cosine similarity function that returns weights which are then used by the read head for associative recall or by the write head to modify the conetent of the memory. In addition, if the key only match a part of the conentent of the memory this is still useful and can lead to the retrieval of that location. This may be due to the key may not have all the information which instead are stored in the memory.\n",
    "\n",
    "#### Temporary links\n",
    "This form keep track of the transitions betweens locations which were consecutively written using an $LxL$ temporal *link matrix* $L$. This matrix associate a weight from 0 to 1 for each pair of locations in the matrix, where the entry $L[i,j]$ is the temporal relation between the location $i$ and $j$. The weight is closer to $1$ if the locations $i$ was written after $j$, otherwise the value is closer to $0$. This gives to the Neural Network the ability to recover sequences following the order under which they were written.\n",
    "<br>\n",
    "\n",
    "The product $L\\mathbf{w}$ creat a smoothing effect, shifting the focus forwards the locations after those emphasized by $\\mathbf{w}$. That is, after a writting which is based on $\\mathbf{w}$\n",
    "\n",
    "#### Usage \n",
    "This form is used to allocate the memory for writting. The usage is a value between $0$ and $1$, with a weighting to select unsued locations that is delivered to the writing head. The usage is incremented after each write to that location and decreased after each read. The good property is that this is independent from the memory size and content. This allows the network to be trained and then upgrade to a larger one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "The code has been taken from [github](https://github.com/deepmind/dnc). The idea is to analyse it and further modify or add some parts of the implementation in order to achieve our goals. For now we just list the major modules which are used to implement the DNC. Then we will used the *Repeat Copy Task* to perfrom some tests on the DNC. The final goal from the implementation point of view is to extend this one with the *Graph Task*\n",
    "<br>\n",
    "The modules provided are the following:\n",
    "- **Addressing and Access  Modules**\n",
    "    The following code implements the addressing methods discussed above. In addition another script provides the function necessary to manipulate the memory exploiting the addressing methods already defined.\n",
    "- **DNC Module**\n",
    "    The following code represents the core of the Differentiable Neural Computer. This is an extention of the class RNN core, which include also the manipulation with the memory. We have to reming that the Controller is implemented as LSTM reccurent network.\n",
    "- **Repeat Copy Task**\n",
    "    The folowing code is used to create a dataset for the copy task. The DNC receives in input batches of collections of string which have to be copied a number of times which is defined randomly.\n",
    "- **Training Module** \n",
    "    Finally the code for the trainig is porvided. This code allows also to save the variables of the model for checkpoint it and restore the values for an input evaluation. Note that the variables are saved and not the model, this mean that it's needed to reacreate the graph then loead the variables during a session.\n",
    "\n",
    "<br>\n",
    "Here follows the code license:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# ==========================================================================\n",
    "# Copyright 2017 Google Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==========================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "TO performs some test on the model we had to modify the way the dataset was originted. The first test is to understand how much the model is sensitive to noised data. We have taken the original generation code and applied some modifications. For random betchs (where the randomness is further explained) we flipped the values of the bit in the string to copy alterning the target. That is, to deceive the DNC to perform it's original task of copying a string of bits several times we have changed the target. So we let the DNC assume it made it wrong and underdtand if for some samples this may cause a non convergence of the error to 0.\n",
    "<br>\n",
    "Before analysizing the noised version we have to test the DNC in a normal copy task with non altered target values. We can exploit the modified code in order to create the training environment over which manipulate the DNC training which allows for both non noised and noised training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat_copy_noise.py\n",
    "Here we show our modified **repeat_copy.py** script for runnin the noised test. A part from some minor changes to the interface, the added parts in the code are highlightes with comments *### MODIFIED ###*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repeat_copy import RepeatCopy\n",
    "\n",
    "DatasetNoisedTensors = collections.namedtuple('DatasetNoisedTensors', ('observations',\n",
    "                                                                       'target',\n",
    "                                                                       'target_noise',\n",
    "                                                                       'mask',\n",
    "                                                                       'distortion'))\n",
    "\n",
    "\n",
    "def custom_bit_string_readable(data, batch_size, model_output=None, whole_batch=False,\n",
    "                               with_noise=False, with_distortion=False):\n",
    "    \"\"\"Produce a human readable representation of the sequences in data.\n",
    "\n",
    "    Args:\n",
    "      data: data to be visualised\n",
    "      batch_size: size of batch\n",
    "      model_output: optional model output tensor to visualize alongside data.\n",
    "      whole_batch: whether to visualise the whole batch. Only a random sample of the\n",
    "          batch will be visualized\n",
    "      with_noise: decide to visualize or not the noised batch\n",
    "      with_distortion: decide to print or not the distortion value computed as cosine\n",
    "          similarity between the real target and the noised one\n",
    "\n",
    "    Returns:\n",
    "      A string used to visualise the data batch\n",
    "    \"\"\"\n",
    "\n",
    "    def _readable(datum):\n",
    "        return '+' + ' '.join(['-' if x == 0 else '%d' % x for x in datum]) + '+'\n",
    "\n",
    "    obs_batch = data.observations\n",
    "    targ_batch = data.target\n",
    "    targ_noise_batch = data.target_noise\n",
    "    dist_batch = data.distortion\n",
    "    iterate_over = range(batch_size) if whole_batch else [random.randint(0, batch_size - 1)]\n",
    "\n",
    "    batch_strings = []\n",
    "    for batch_index in iterate_over:\n",
    "        obs = obs_batch[:, batch_index, :]\n",
    "        targ = targ_batch[:, batch_index, :]\n",
    "        targ_n = targ_noise_batch[:, batch_index, :]\n",
    "        dist = dist_batch[batch_index]\n",
    "\n",
    "        obs_channels = range(obs.shape[1])\n",
    "        targ_channels = range(targ.shape[1])\n",
    "        obs_channel_strings = [_readable(obs[:, i]) for i in obs_channels]\n",
    "        targ_channel_strings = [_readable(targ[:, i]) for i in targ_channels]\n",
    "        targ_noise_channel_strings = [_readable(targ_n[:, i]) for i in targ_channels]\n",
    "\n",
    "        readable_obs = 'Observations:\\n' + '\\n'.join(obs_channel_strings)\n",
    "        readable_targ = 'Targs:\\n' + '\\n'.join(targ_channel_strings)\n",
    "        readable_targ_n = 'Target_noise: \\n' + '\\n'.join(targ_noise_channel_strings)\n",
    "        readable_dist = 'Distortion {}\\n'.format(dist)\n",
    "\n",
    "        if with_noise and with_distortion:\n",
    "            strings = [readable_obs, readable_targ, readable_targ_n, readable_dist]\n",
    "        elif with_distortion and not with_distortion:\n",
    "            strings = [readable_obs, readable_targ, readable_dist]\n",
    "        elif with_noise and not with_distortion:\n",
    "            strings = [readable_obs, readable_targ, readable_targ_n]\n",
    "        else:\n",
    "            strings = [readable_obs, readable_targ]\n",
    "\n",
    "        if model_output is not None:\n",
    "            output = model_output[:, batch_index, :]\n",
    "            output_strings = [_readable(output[:, i]) for i in targ_channels]\n",
    "            strings.append('Model Output:\\n' + '\\n'.join(output_strings))\n",
    "\n",
    "        batch_strings.append('\\n\\n'.join(strings))\n",
    "\n",
    "    return '\\n' + '\\n\\n\\n\\n'.join(batch_strings)\n",
    "\n",
    "\n",
    "class NoisedRepeatCopy(RepeatCopy):\n",
    "\n",
    "    def __init__(self, num_bits=6, batch_size=1, min_length=1, max_length=1, min_repeats=1, max_repeats=2, \n",
    "                 norm_max=10, log_prob_in_bits=False, time_average_cost=False, \n",
    "                 name='repeat_copy', noise_level=None):\n",
    "        super(NoisedRepeatCopy, self).__init__(num_bits, batch_size, min_length, max_length, min_repeats, \n",
    "                                               max_repeats, norm_max, log_prob_in_bits, time_average_cost, name)\n",
    "\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def _build(self):\n",
    "        min_length, max_length = self._min_length, self._max_length\n",
    "        min_reps, max_reps = self._min_repeats, self._max_repeats\n",
    "        num_bits = self.num_bits\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        full_obs_size = num_bits + 2\n",
    "        full_targ_size = num_bits + 1\n",
    "        start_end_flag_idx = full_obs_size - 2\n",
    "        num_repeats_channel_idx = full_obs_size - 1\n",
    "\n",
    "        sub_seq_length_batch = tf.random_uniform(\n",
    "            [batch_size], minval=min_length, maxval=max_length + 1, dtype=tf.int32)\n",
    "        num_repeats_batch = tf.random_uniform(\n",
    "            [batch_size], minval=min_reps, maxval=max_reps + 1, dtype=tf.int32)\n",
    "\n",
    "        total_length_batch = sub_seq_length_batch * (num_repeats_batch + 1) + 3\n",
    "        max_length_batch = tf.reduce_max(total_length_batch)\n",
    "        residual_length_batch = max_length_batch - total_length_batch\n",
    "\n",
    "        obs_batch_shape = [max_length_batch, batch_size, full_obs_size]\n",
    "        targ_batch_shape = [max_length_batch, batch_size, full_targ_size]\n",
    "        mask_batch_trans_shape = [batch_size, max_length_batch]\n",
    "\n",
    "        obs_tensors = []\n",
    "        targ_tensors = []\n",
    "        # include also the noised version\n",
    "        targ_noise_tensors = []\n",
    "        mask_tensors = []\n",
    "        # distortion\n",
    "        distortion = []\n",
    "\n",
    "        ### MODIFIED FROM HERE ######\n",
    "\n",
    "        for batch_index in range(batch_size):\n",
    "            sub_seq_len = sub_seq_length_batch[batch_index]\n",
    "            num_reps = num_repeats_batch[batch_index]\n",
    "\n",
    "            obs_pattern_shape = [sub_seq_len, num_bits]\n",
    "            obs_pattern = tf.cast(\n",
    "                tf.random_uniform(\n",
    "                    obs_pattern_shape, minval=0, maxval=2, dtype=tf.int32),\n",
    "                tf.float32)\n",
    "\n",
    "            targ_pattern_shape = [sub_seq_len * num_reps, num_bits]\n",
    "            flat_obs_pattern = tf.reshape(obs_pattern, [-1])\n",
    "            flat_targ_pattern = tf.tile(flat_obs_pattern, tf.stack([num_reps]))\n",
    "\n",
    "            # perturbation of the target\n",
    "            def addNoise(x):\n",
    "                #print('Added Noise'.format(batch_index))\n",
    "                val = np.random.randint(1, 100)\n",
    "\n",
    "                if (self.noise_level is not None) and (val > self.noise_level):\n",
    "                    #print('Noise applied!', val)\n",
    "\n",
    "                    def f1():\n",
    "                        return tf.add(x, 1)\n",
    "\n",
    "                    def f2():\n",
    "                        return tf.add(x, -1)\n",
    "\n",
    "                    output = tf.cond(tf.equal(x, tf.constant(1, dtype=tf.float32)), true_fn=f2, false_fn=f1)\n",
    "                    return output   \n",
    "                else:\n",
    "                    return x\n",
    "\n",
    "            # create a noised version of the target\n",
    "            noised_flat_targ_pattern = tf.map_fn(addNoise, flat_targ_pattern)\n",
    "\n",
    "            ### EVALUATE THE DISTORTION\n",
    "            # compute how many bits are changed as a distortion metric\n",
    "            distortion.append(\n",
    "                tf.reduce_sum(\n",
    "                    tf.square(\n",
    "                        tf.subtract(\n",
    "                            noised_flat_targ_pattern, flat_targ_pattern\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # reshape the tensors (both the original and the noised one)\n",
    "            targ_pattern = tf.reshape(flat_targ_pattern, targ_pattern_shape)\n",
    "            target_noise_pattern = tf.reshape(noised_flat_targ_pattern, targ_pattern_shape)\n",
    "\n",
    "            ##### END MODIFICATIONS\n",
    "\n",
    "            obs_flag_channel_pad = tf.zeros([sub_seq_len, 2])\n",
    "            obs_start_flag = tf.one_hot(\n",
    "                [start_end_flag_idx], full_obs_size, on_value=1., off_value=0.)\n",
    "            num_reps_flag = tf.one_hot(\n",
    "                [num_repeats_channel_idx],\n",
    "                full_obs_size,\n",
    "                on_value=self._normalise(tf.cast(num_reps, tf.float32)),\n",
    "                off_value=0.)\n",
    "\n",
    "            # note the concatenation dimensions.\n",
    "            obs = tf.concat([obs_pattern, obs_flag_channel_pad], 1)\n",
    "            obs = tf.concat([obs_start_flag, obs], 0)\n",
    "            obs = tf.concat([obs, num_reps_flag], 0)\n",
    "\n",
    "            # Now do the same for the targ_pattern (it only has one extra channel).\n",
    "            targ_flag_channel_pad = tf.zeros([sub_seq_len * num_reps, 1])\n",
    "            targ_end_flag = tf.one_hot(\n",
    "                [start_end_flag_idx], full_targ_size, on_value=1., off_value=0.)\n",
    "            targ = tf.concat([targ_pattern, targ_flag_channel_pad], 1)\n",
    "            targ = tf.concat([targ, targ_end_flag], 0)\n",
    "\n",
    "            ### INCLUDE THE NOISED ONE\n",
    "            targ_noise = tf.concat([target_noise_pattern, targ_flag_channel_pad], 1)\n",
    "            targ_noise = tf.concat([targ_noise, targ_end_flag], 0)\n",
    "            ###\n",
    "\n",
    "            # This aligns them s.t. the target begins as soon as the obs ends.\n",
    "            obs_end_pad = tf.zeros([sub_seq_len * num_reps + 1, full_obs_size])\n",
    "            targ_start_pad = tf.zeros([sub_seq_len + 2, full_targ_size])\n",
    "\n",
    "            # The mask is zero during the obs and one during the targ.\n",
    "            mask_off = tf.zeros([sub_seq_len + 2])\n",
    "            mask_on = tf.ones([sub_seq_len * num_reps + 1])\n",
    "\n",
    "            obs = tf.concat([obs, obs_end_pad], 0)\n",
    "            targ = tf.concat([targ_start_pad, targ], 0)\n",
    "\n",
    "            ### INCLUDE THE NOISED ONE\n",
    "            targ_noise = tf.concat([targ_start_pad, targ_noise], 0)\n",
    "            ###\n",
    "\n",
    "            mask = tf.concat([mask_off, mask_on], 0)\n",
    "\n",
    "            obs_tensors.append(obs)\n",
    "            targ_tensors.append(targ)\n",
    "\n",
    "            ### INCLUDE THE NOISED ONE\n",
    "            targ_noise_tensors.append(targ_noise)\n",
    "            ###\n",
    "\n",
    "            mask_tensors.append(mask)\n",
    "\n",
    "        # End the loop over batch index.\n",
    "        # Compute how much zero padding is needed to make tensors sequences\n",
    "        # the same length for all batch elements.\n",
    "        residual_obs_pad = [\n",
    "            tf.zeros([residual_length_batch[i], full_obs_size])\n",
    "            for i in range(batch_size)\n",
    "        ]\n",
    "        residual_targ_pad = [\n",
    "            tf.zeros([residual_length_batch[i], full_targ_size])\n",
    "            for i in range(batch_size)\n",
    "        ]\n",
    "        residual_mask_pad = [\n",
    "            tf.zeros([residual_length_batch[i]]) for i in range(batch_size)\n",
    "        ]\n",
    "\n",
    "        # Concatenate the pad to each batch element.\n",
    "        obs_tensors = [\n",
    "            tf.concat([o, p], 0) for o, p in zip(obs_tensors, residual_obs_pad)\n",
    "        ]\n",
    "        targ_tensors = [\n",
    "            tf.concat([t, p], 0) for t, p in zip(targ_tensors, residual_targ_pad)\n",
    "        ]\n",
    "\n",
    "        ### INCLUDE THE NOISED ONE\n",
    "        targ_noise_tensors = [\n",
    "            tf.concat([t, p], 0) for t, p in zip(targ_noise_tensors, residual_targ_pad)\n",
    "        ]\n",
    "        ###\n",
    "\n",
    "        mask_tensors = [\n",
    "            tf.concat([m, p], 0) for m, p in zip(mask_tensors, residual_mask_pad)\n",
    "        ]\n",
    "\n",
    "        # Concatenate each batch element into a single tensor.\n",
    "        obs = tf.reshape(tf.concat(obs_tensors, 1), obs_batch_shape)\n",
    "        targ = tf.reshape(tf.concat(targ_tensors, 1), targ_batch_shape)\n",
    "\n",
    "        ### INCLUDE THE NOISED ONE\n",
    "        targ_noise = tf.reshape(tf.concat(targ_noise_tensors, 1), targ_batch_shape)\n",
    "        ###\n",
    "\n",
    "        mask = tf.transpose(\n",
    "            tf.reshape(tf.concat(mask_tensors, 0), mask_batch_trans_shape))\n",
    "        # return the collection including the noised one\n",
    "        return DatasetNoisedTensors(obs, targ, targ_noise, mask, distortion)\n",
    "\n",
    "    def to_human_readable(self, data, model_output=None, whole_batch=False, with_distortion=False, \n",
    "                          with_noise=False):\n",
    "        obs = data.observations\n",
    "        # it has to denormalize the value associated with the channel for the number of repetitions\n",
    "        # that is why it takes the last row\n",
    "        unnormalised_num_reps_flag = self._unnormalise(obs[:, :, -1:]).round()\n",
    "        # rebuild the original one with the unormalized values\n",
    "        obs = np.concatenate([obs[:, :, :-1], unnormalised_num_reps_flag], axis=2)\n",
    "        data = data._replace(observations=obs)\n",
    "        return custom_bit_string_readable(data, self.batch_size, model_output, whole_batch, with_noise, \n",
    "                                          with_distortion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_noise.py\n",
    "Here we show our modified version of **train.py** which exploits the modified version of repeat_copy.py to perform the training task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "HIDDEN_SIZE = 64\n",
    "MEMORY_SIZE = 16\n",
    "WORD_SIZE = 16\n",
    "NUM_WRITE_HEADS = 1\n",
    "NUM_READ_HEADS = 4\n",
    "CLIP_VALUE = 20\n",
    "\n",
    "# Optimizer parameters.\n",
    "MAX_GRAD_NORM = 50\n",
    "LEARN_RATE = 1e-4\n",
    "OPTIMIZER_EPSY = 1e-10\n",
    "\n",
    "# Task parameters\n",
    "BATCH_SIZE = 16\n",
    "NUMB_BITS = 4\n",
    "MIN_LENGTH = 1\n",
    "MAX_LENGTH = 2\n",
    "MIN_REPEATS = 1\n",
    "MAX_REPEATS = 2\n",
    "\n",
    "# Training options.\n",
    "TRAINING_ITER = 1000\n",
    "REPORT_INTERV = 100\n",
    "CHECHK_POINT_DIR = \"/tmp/tf/dnc\"\n",
    "CHECK_POINT_INTERVAL = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnc import DNC\n",
    "\n",
    "class Training():\n",
    "    \n",
    "    def __init__(self, hidden_size, memory_size, word_size, num_write_heads, num_read_heads, clip_value, \n",
    "                max_grad_norm, learning_rate, optimizier_epsy, batch_size, numb_bits, min_length, max_length, \n",
    "                min_repeats, max_repeats):\n",
    "        \n",
    "        self._hidden_size = hidden_size \n",
    "        self._memory_size = memory_size\n",
    "        self._word_size = word_size\n",
    "        self._num_write_heads = num_write_heads\n",
    "        self._num_read_heads = num_read_heads\n",
    "        self._clip_value = clip_value\n",
    "\n",
    "        self._max_grad_norm = max_grad_norm\n",
    "        self._learning_rate = learning_rate\n",
    "        self._optimizier_epsy = optimizier_epsy\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._numb_bits = numb_bits\n",
    "        self._min_length = min_length\n",
    "        self._max_length = max_length\n",
    "        self._min_repeats = min_repeats\n",
    "        self._max_repeats = max_repeats\n",
    "  \n",
    "\n",
    "    def run_model(self, input_sequence, output_size):\n",
    "        \"\"\"Runs model on input sequence.\"\"\"\n",
    "\n",
    "        access_config = {\n",
    "            \"memory_size\": self._memory_size,\n",
    "            \"word_size\": self._word_size,\n",
    "            \"num_reads\": self._num_read_heads,\n",
    "            \"num_writes\": self._num_write_heads,\n",
    "        }\n",
    "        controller_config = {\n",
    "            \"hidden_size\": self._hidden_size,\n",
    "        }\n",
    "        clip_value = self._clip_value\n",
    "\n",
    "        dnc_core = DNC(access_config, controller_config, output_size, clip_value)\n",
    "        initial_state = dnc_core.initial_state(self._batch_size)\n",
    "\n",
    "        output_sequence, _ = tf.nn.dynamic_rnn(\n",
    "            # instance of a RNN core module\n",
    "            cell=dnc_core,\n",
    "            inputs=input_sequence,\n",
    "            time_major=True,\n",
    "            initial_state=initial_state)\n",
    "\n",
    "        return output_sequence\n",
    "\n",
    "\n",
    "    def train(self, num_training_iterations, report_interval, checkpoint_dir='/tmp/tf/dnc', checkpoint_interval=-1, \n",
    "              with_distorsion=False, with_noise=False, noise_level=None, verbosity=0):\n",
    "        \"\"\"Trains the DNC and periodically reports the loss.\"\"\"\n",
    "\n",
    "\n",
    "        #  dataset definition\n",
    "        dataset_noise = NoisedRepeatCopy(self._numb_bits, self._batch_size,\n",
    "                                                           self._min_length, self._max_length,\n",
    "                                                           self._min_repeats, self._max_repeats,\n",
    "                                                           noise_level=noise_level)\n",
    "\n",
    "        dataset_tensors = dataset_noise()\n",
    "\n",
    "        #  compute the output of the RNN\n",
    "        output_logits = self.run_model(dataset_tensors.observations, dataset_noise.target_size)\n",
    "        # Used for visualization\n",
    "        output = tf.round(\n",
    "            tf.expand_dims(dataset_tensors.mask, -1) * tf.sigmoid(output_logits))\n",
    "\n",
    "        #  compute the loss with the respect of the target\n",
    "        train_loss = dataset_noise.cost(output_logits, dataset_tensors.target_noise,\n",
    "                                        dataset_tensors.mask)\n",
    "\n",
    "        # Set up optimizer with global norm clipping.\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "\n",
    "        #  compute the gradient\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(train_loss, trainable_variables), self._max_grad_norm)\n",
    "\n",
    "        # define a global variable\n",
    "        global_step = tf.get_variable(\n",
    "            name=\"global_step\",\n",
    "            shape=[],\n",
    "            dtype=tf.int64,\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            trainable=False,\n",
    "            collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n",
    "\n",
    "        #  run the backward gradient propagation\n",
    "        optimizer = tf.train.RMSPropOptimizer(\n",
    "            self._learning_rate, epsilon=self._optimizier_epsy)\n",
    "        # GLOBAL -> this is incremented by one after the minimization (backward) have been executed\n",
    "        train_step = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables), global_step=global_step)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        if checkpoint_interval > 0:\n",
    "            hooks = [\n",
    "                tf.train.CheckpointSaverHook(\n",
    "                    checkpoint_dir=checkpoint_dir,\n",
    "                    save_steps=checkpoint_interval,\n",
    "                    saver=saver)\n",
    "            ]\n",
    "        else:\n",
    "            hooks = []\n",
    "\n",
    "        with tf.train.SingularMonitoredSession(\n",
    "                hooks=hooks, checkpoint_dir=checkpoint_dir) as sess:\n",
    "\n",
    "            start_iteration = sess.run(global_step)\n",
    "            tf.logging.info(\" - Training has started!\")\n",
    "\n",
    "            results = {\n",
    "                'noise_level': NOISE_LEVEL,\n",
    "                'iteration': [],\n",
    "                'outputs': [],\n",
    "                'data': [],\n",
    "                'strings': [],\n",
    "                'losses': []\n",
    "            }\n",
    "\n",
    "            total_loss = 0\n",
    "            for train_iteration in range(start_iteration, num_training_iterations):\n",
    "                _, loss = sess.run([train_step, train_loss])\n",
    "                total_loss += loss\n",
    "\n",
    "                if (train_iteration + 1) % report_interval == 0:\n",
    "                    dataset_tensors_np, output_np = sess.run([dataset_tensors, output])\n",
    "                    dataset_string = dataset_noise.to_human_readable(dataset_tensors_np, output_np,\n",
    "                                                                     with_distortion=with_distorsion,\n",
    "                                                                     with_noise=with_noise)\n",
    "\n",
    "                    results['iteration'].append(train_iteration)\n",
    "                    results['data'].append(dataset_tensors_np)\n",
    "                    results['strings'].append(dataset_string)\n",
    "                    results['outputs'].append(output_np)\n",
    "                    results['losses'].append(total_loss / report_interval)\n",
    "\n",
    "                    if verbosity is 0:\n",
    "                        tf.logging.info(\" - Iter: %d: Avg training loss %f.\",\n",
    "                                        train_iteration, total_loss / report_interval)\n",
    "                    else:\n",
    "                        tf.logging.info(\" - Iter: %d: Avg training loss %f.\\n%s\",\n",
    "                                        train_iteration, total_loss / report_interval,\n",
    "                                        dataset_string)\n",
    "                    total_loss = 0\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 16, 5), dtype=float32)\n",
      "Tensor(\"repeat_copy/Reshape_50:0\", shape=(?, 16, 5), dtype=float32)\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow: - Training has started!\n",
      "INFO:tensorflow: - Iter: 4999: Avg training loss 7.938283.\n",
      "INFO:tensorflow: - Iter: 9999: Avg training loss 4.396687.\n",
      "INFO:tensorflow: - Iter: 14999: Avg training loss 2.364800.\n",
      "INFO:tensorflow: - Iter: 19999: Avg training loss 1.378604.\n",
      "INFO:tensorflow: - Iter: 24999: Avg training loss 0.773323.\n",
      "INFO:tensorflow: - Iter: 29999: Avg training loss 0.333952.\n",
      "INFO:tensorflow: - Iter: 34999: Avg training loss 0.185517.\n",
      "INFO:tensorflow: - Iter: 39999: Avg training loss 0.127265.\n",
      "INFO:tensorflow: - Iter: 44999: Avg training loss 0.096554.\n",
      "INFO:tensorflow: - Iter: 49999: Avg training loss 0.080684.\n",
      "INFO:tensorflow: - Iter: 54999: Avg training loss 0.060898.\n",
      "INFO:tensorflow: - Iter: 59999: Avg training loss 0.051502.\n",
      "INFO:tensorflow: - Iter: 64999: Avg training loss 0.051568.\n",
      "INFO:tensorflow: - Iter: 69999: Avg training loss 0.054861.\n",
      "INFO:tensorflow: - Iter: 74999: Avg training loss 0.048359.\n",
      "INFO:tensorflow: - Iter: 79999: Avg training loss 0.054208.\n",
      "INFO:tensorflow: - Iter: 84999: Avg training loss 0.049640.\n",
      "INFO:tensorflow: - Iter: 89999: Avg training loss 0.037666.\n",
      "INFO:tensorflow: - Iter: 94999: Avg training loss 0.040014.\n",
      "INFO:tensorflow: - Iter: 99999: Avg training loss 0.026952.\n"
     ]
    }
   ],
   "source": [
    "NOISE_LEVEL = None\n",
    "TRAINING_ITER = 100000\n",
    "REPORT_INTERV = 5000\n",
    "\n",
    "training = Training(HIDDEN_SIZE, MEMORY_SIZE, WORD_SIZE, NUM_WRITE_HEADS, NUM_READ_HEADS, \n",
    "                   CLIP_VALUE, MAX_GRAD_NORM, LEARN_RATE, OPTIMIZER_EPSY, BATCH_SIZE, \n",
    "                   NUMB_BITS, MIN_LENGTH, MAX_LENGTH, MIN_REPEATS, MAX_REPEATS)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.logging.set_verbosity(3)  # Print INFO log messages.\n",
    "results_clean = training.train(TRAINING_ITER, REPORT_INTERV, with_noise=False, with_distorsion=False, \n",
    "                               noise_level=NOISE_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 16, 5), dtype=float32)\n",
      "Tensor(\"repeat_copy/Reshape_50:0\", shape=(?, 16, 5), dtype=float32)\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow: - Training has started!\n",
      "INFO:tensorflow: - Iter: 4999: Avg training loss 8.309622.\n",
      "INFO:tensorflow: - Iter: 9999: Avg training loss 5.732491.\n",
      "INFO:tensorflow: - Iter: 14999: Avg training loss 3.877566.\n",
      "INFO:tensorflow: - Iter: 19999: Avg training loss 3.107155.\n",
      "INFO:tensorflow: - Iter: 24999: Avg training loss 2.822140.\n",
      "INFO:tensorflow: - Iter: 29999: Avg training loss 2.637930.\n",
      "INFO:tensorflow: - Iter: 34999: Avg training loss 2.483084.\n",
      "INFO:tensorflow: - Iter: 39999: Avg training loss 2.352814.\n",
      "INFO:tensorflow: - Iter: 44999: Avg training loss 2.306345.\n",
      "INFO:tensorflow: - Iter: 49999: Avg training loss 2.259139.\n",
      "INFO:tensorflow: - Iter: 54999: Avg training loss 2.233495.\n",
      "INFO:tensorflow: - Iter: 59999: Avg training loss 2.212310.\n",
      "INFO:tensorflow: - Iter: 64999: Avg training loss 2.206566.\n",
      "INFO:tensorflow: - Iter: 69999: Avg training loss 2.188561.\n",
      "INFO:tensorflow: - Iter: 74999: Avg training loss 2.205978.\n",
      "INFO:tensorflow: - Iter: 79999: Avg training loss 2.203298.\n",
      "INFO:tensorflow: - Iter: 84999: Avg training loss 2.184547.\n",
      "INFO:tensorflow: - Iter: 89999: Avg training loss 2.174551.\n",
      "INFO:tensorflow: - Iter: 94999: Avg training loss 2.171541.\n",
      "INFO:tensorflow: - Iter: 99999: Avg training loss 2.164614.\n"
     ]
    }
   ],
   "source": [
    "NOISE_LEVEL = 85.\n",
    "TRAINING_ITER = 100000\n",
    "REPORT_INTERV = 5000\n",
    "\n",
    "training = Training(HIDDEN_SIZE, MEMORY_SIZE, WORD_SIZE, NUM_WRITE_HEADS, NUM_READ_HEADS, \n",
    "                   CLIP_VALUE, MAX_GRAD_NORM, LEARN_RATE, OPTIMIZER_EPSY, BATCH_SIZE, \n",
    "                   NUMB_BITS, MIN_LENGTH, MAX_LENGTH, MIN_REPEATS, MAX_REPEATS)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.logging.set_verbosity(3)  # Print INFO log messages.\n",
    "results_noise = training.train(TRAINING_ITER, REPORT_INTERV, with_noise=False, with_distorsion=False, \n",
    "                               noise_level=NOISE_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This *matrix* shows the number of changed bits. It has to be red as follow: there is one row per iteration saved and one column for each batch. Take into account that for each iteration 16 batched are generated, in this case only one has been affected by noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for indx in range(len(results_noise['data'])):\n",
    "    print(results_noise['data'][indx][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here instead we show the result for each saved iteration. It's possible to observe the observation, which is the input of the DNC and the model output with the respect of the target. To be noticed, the last two rows in the observation are the termination channel and the repetition channel respectively. For the model output the last row is the termination channel. Those channel are not part of the array to be copied but just extra information passed to the DNC conroller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Iteration: 4999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 8.309621811771393\n",
      ">>>> Iteration: 9999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      ">>>> Losses: 5.732491242980957\n",
      ">>>> Iteration: 14999\n",
      "\n",
      "Observations:\n",
      "+- 1 1 - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - - 1 - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - - 1 - -+\n",
      ">>>> Losses: 3.8775656994581222\n",
      ">>>> Iteration: 19999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 3.1071551090717318\n",
      ">>>> Iteration: 24999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 1 - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 2 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - 1 1 1 1 -+\n",
      "+- - - - - 1 - 1 -+\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - - - - - 1+\n",
      "\n",
      "Model Output:\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - 1 1 1 1 -+\n",
      "+- - - - - 1 - 1 -+\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - - - - - 1+\n",
      ">>>> Losses: 2.8221403005599974\n",
      ">>>> Iteration: 29999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 2.6379295472860336\n",
      ">>>> Iteration: 34999\n",
      "\n",
      "Observations:\n",
      "+- 1 1 - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 2 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - 1 1 1 1 -+\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - 1+\n",
      "\n",
      "Model Output:\n",
      "+- - - - 1 1 1 1 -+\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - 1+\n",
      ">>>> Losses: 2.483083514547348\n",
      ">>>> Iteration: 39999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 2.352814322423935\n",
      ">>>> Iteration: 44999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 2 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - 1 -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - 1+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - 1 -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - 1+\n",
      ">>>> Losses: 2.3063452721595765\n",
      ">>>> Iteration: 49999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      ">>>> Losses: 2.2591393342256545\n",
      ">>>> Iteration: 54999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 1 - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 2 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - 1 1 1 1 -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - 1+\n",
      "\n",
      "Model Output:\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - 1 1 1 1 -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - 1+\n",
      ">>>> Losses: 2.233495108151436\n",
      ">>>> Iteration: 59999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 2.2123097828388216\n",
      ">>>> Iteration: 64999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - -+\n",
      "+- - - - - - -+\n",
      "+- - - - - - -+\n",
      "+- 1 - - - - -+\n",
      "+1 - - - - - -+\n",
      "+- - 1 - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - 1 - - -+\n",
      "+- - - - - - -+\n",
      "+- - - - - - -+\n",
      "+- - - 1 - - -+\n",
      "+- - - - 1 - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - 1 - - -+\n",
      "+- - - - - - -+\n",
      "+- - - - - - -+\n",
      "+- - - 1 - - -+\n",
      "+- - - - 1 - -+\n",
      ">>>> Losses: 2.206565974211693\n",
      ">>>> Iteration: 69999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 2.1885611533641813\n",
      ">>>> Iteration: 74999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 2.205977590751648\n",
      ">>>> Iteration: 79999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "+- 1 1 - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - - 1 - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - - 1 - -+\n",
      ">>>> Losses: 2.203298367023468\n",
      ">>>> Iteration: 84999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      ">>>> Losses: 2.184546591448784\n",
      ">>>> Iteration: 89999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 1 - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "+- 1 1 - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - - 1 - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - - - 1 - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - - 1 - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - - - 1 - -+\n",
      ">>>> Losses: 2.1745511697769166\n",
      ">>>> Iteration: 94999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- 1 1 - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - 1 - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - 1 - -+\n",
      ">>>> Losses: 2.171540751314163\n",
      ">>>> Iteration: 99999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 2.164613728427887\n"
     ]
    }
   ],
   "source": [
    "for string, iteration, loss in zip(results_noise['strings'], \n",
    "                                   results_noise['iteration'],\n",
    "                                   results_noise['losses']):\n",
    "    print('>>>> Iteration: ' + str(iteration))\n",
    "    print(string)\n",
    "    print('>>>> Losses: '+ str(loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "It's possible to see how the noised version is slower in converge to a zero error. We should look for longer training iterations in order to see how much it will improve and if this has some noticeble effects on the copy task. Indeed, for now, even in the case of some noise, the DNC was able to copy the vector without errors in the observed iterations. Further, we should investigate different velues of the noise and different parameter of the task (like increasing the size of the array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
