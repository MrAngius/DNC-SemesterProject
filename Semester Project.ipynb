{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semester Project:\n",
    "# exploring Differentiable Neural Computers and applications\n",
    "\n",
    "## Introduction\n",
    "What we want to present in the following notebook is a cutting-edge topic in machine learning, related to a particular model of Neural Network called **Differentable Neural Computer** (DNC) which puts together a Neural Network and an external memory from which the Netwoek can read and write. The main source of this noteboook is based on the article and paper which can be found [here](https://deepmind.com/blog/differentiable-neural-computers/).\n",
    "<br>\n",
    "\n",
    "We start with a presentation of the DNC and how they are enhanced form *Neural Turing Machines*. Then we move to the core topic of this networks which are the three **differentiable attention mechanisms** with whihc the network performs the read and writes operations. We propose then a code for the implementation of these Networks, testing their performances and limitations. Finally we will try to implement the graph task presented in the paper and adapt it to a different problem.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNC\n",
    "The Differentiable Neural Computers are neural networks with a coupled memory, where the dimension of this memory does not affect the behaviour of the network. The memory can be tought as a RAM and the Network as a controller which is differentiable using gradient descent-like tecniques.\n",
    "<br>\n",
    "Comparing this Network with the respect of other whihc present memory states such as RNN or LSTM, DNCs do not store information whihc is tigth coupled with the model itself. Instead, they have the possibility to selectively read and write to and from memory locations, creating a separation between state preservation and content (data).\n",
    "<br>\n",
    "\n",
    "In the litterature, another network capable of read and write using an external memory came out in the past, called **Neural Turing Machine** (NTM). Altought the concept is similar to the DNC, with a Network acting as controller and an external memory, they are more restricted in the method with which the Neural Network access the memory. \n",
    "\n",
    "<br>\n",
    "![DNC](./figures/DNC_architecture.png)\n",
    "<br>\n",
    "\n",
    "The image shows the architecture of a DNC Neural Network. In this specif example, the controller is a Recursive Neural Network (a LSTM is also possible) which is receiving an input (from a dataset) and the previous step readings from the memory. The output of the network is made of two parts:\n",
    "- The output of the Neural Network, which constitute our target\n",
    "- A vector called **'Interface Vector'** which is used by the read and write heads to interact with the memory\n",
    "\n",
    "As it's clear by now, the DNC is mainly composed of two parts interacting together: the **Neural Netwoerk** itself (**a**) and the **Heads** (**b**) which performs the operations over the **Memory** (**c**). Those oprations are the key differences of DNC over NTM and will be described in details in the next paragraph. \n",
    "\n",
    "The Heads are particular components which are dealing with the content of the memory. It's possible to define as many read and write heads as needed, all of them will receive weights vector which are used to define the location over which perform the read and write operations. The differentiability of these parts allows the network to learn how to perform these operations simple by looking at the error during the trainign process and adjusting the weights.\n",
    "\n",
    "Finally, in addition to the Memory there is an additional information whcih is saved and used during the read and writes operations. Those are the **Memory Usage and Temporal Links** (**d**) which is an imprvement over the NTM. Those links and usage information allows to dynamically allocate the content during the reads and to have notion of temporal assosciation between entries of the memory. That is, it's possibile to know the sequence of the writtings which is extremely important when the Network has to deal to sequential tasks such as graph path search.\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Interactions\n",
    "\n",
    "### Overview\n",
    "The core mechanism of DNCs is the possibility to write and read from an external memory matrix. As mentioned, the difference with the NTMs is in defining more **attention mechanisms**. This is different from the address mechanisms in conventional computers, where there is a mapping between the address and the content of the memory. The idea here is to define the weights over the location of the Memory. Those weights represents a degree (we can immagine it has a filter) that indicates how much the locations in the Memory matrix are involved during read and writing operations.\n",
    "<br>\n",
    "Briefly:\n",
    "- a read vector $\\mathbf{r}$ is returned after a read operation which involves a read weights matrix $\\mathbf{w^r}$:<br>\n",
    "    $\\mathbf{r}=\\sum_{i=1}^N M[i,j]w^r[i]$ for $j=1, \\dots,W$\n",
    "    \n",
    "- an erase vector $\\mathbf{e}$ is applied using the write weights matrix $\\mathbf{w^w}$, then a a write vector is added $\\mathbf{v}$:<br>\n",
    "    $\\mathbf{v}: M[i,j] \\leftarrow M[i,j](1-\\mathbf{w^w}[i]\\mathbf{e}[j]+\\mathbf{w^w}[i]\\mathbf{v}[j])$\n",
    "\n",
    "The units which determines this operations are the read and write Heads. \n",
    "\n",
    "### Differentiable Attention\n",
    "There are three forms of differentiable attention. This is more a fancy terminology to call the three pricipal techniques involved in in order to address the memory. Before going into the details we want to recall some of the techniques which are instead used in NTM to appreasciate the difference with the DNC. \n",
    "\n",
    "There are two mechanism for addressing the memory and the Neural Tuning Machines combines both:\n",
    "- *content-based addressing*: focuses attention on locations based on the similarity between their current values and values emitted by the controller. This is related to the content addressing of the Hopfield networks, the controller needs to generate a value which is an approximation of the one stored to then retrieve the exact location. \n",
    "- *location-based addressing*: it's the traditional approach to address the memory. For arithmetic operation where we need to define variables the conent based in not enough, we need the location of the variable to perform the operation.\n",
    "\n",
    "Differentiable Neural Computers uses a *content-based addressing* paired with other two techniques which allows for a **Dynamic Memory Addressing** counteracting the major drawbacks of the NTM:\n",
    "1. NTMs do not avoid possible overlapping and interfere among blocks of allocating memory. DNCs instead overcame this problem due to there is only a single free at each write time.\n",
    "2. NTMs does not allow for freeing location of memory which are not used and this can be a problem when processing long sequences. DNCs instead can free memory locations based on the usage weights.\n",
    "3. NTMs sequential information is preserved only if the content is written in consecutive locations. DNCs uses an additional temporal link matrix avoidin the restriction to continuous locations only. \n",
    "\n",
    "For now we explain the concepts of these three differentiable attention. A more detailed analysis will be given in the following paragraphs\n",
    "\n",
    "#### Content base addressing\n",
    "This form is used to determine the similarity measure between the vector emmited by the controller and the content of the memory. The measure is a cosine similarity function that returns weights which are then used by the read head for associative recall or by the write head to modify the conetent of the memory. In addition, if the key only match a part of the conentent of the memory this is still useful and can lead to the retrieval of that location. This may be due to the key may not have all the information which instead are stored in the memory.\n",
    "\n",
    "#### Temporary links\n",
    "This form keep track of the transitions betweens locations which were consecutively written using an $LxL$ temporal *link matrix* $L$. This matrix associate a weight from 0 to 1 for each pair of locations in the matrix, where the entry $L[i,j]$ is the temporal relation between the location $i$ and $j$. The weight is closer to $1$ if the locations $i$ was written after $j$, otherwise the value is closer to $0$. This gives to the Neural Network the ability to recover sequences following the order under which they were written.\n",
    "<br>\n",
    "\n",
    "The product $L\\mathbf{w}$ creat a smoothing effect, shifting the focus forwards the locations after those emphasized by $\\mathbf{w}$. That is, after a writting which is based on $\\mathbf{w}$\n",
    "\n",
    "#### Usage \n",
    "This form is used to allocate the memory for writting. The usage is a value between $0$ and $1$, with a weighting to select unsued locations that is delivered to the writing head. The usage is incremented after each write to that location and decreased after each read. The good property is that this is independent from the memory size and content. This allows the network to be trained and then upgrade to a larger one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details\n",
    "### TODO\n",
    "Includere i dettagli della DNC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "The code has been taken from [github](https://github.com/deepmind/dnc). The idea is to analyse it and further modify or add some parts of the implementation in order to achieve our goals. For now we just list the major modules which are used to implement the DNC. Then we will used the *Repeat Copy Task* to perfrom some tests on the DNC. The final goal from the implementation point of view is to extend this one with the *Graph Task*\n",
    "<br>\n",
    "The modules provided are the following:\n",
    "- **Addressing and Access  Modules**\n",
    "    The following code implements the addressing methods discussed above. In addition another script provides the function necessary to manipulate the memory exploiting the addressing methods already defined.\n",
    "- **DNC Module**\n",
    "    The following code represents the core of the Differentiable Neural Computer. This is an extention of the class RNN core, which include also the manipulation with the memory. We have to reming that the Controller is implemented as LSTM reccurent network.\n",
    "- **Repeat Copy Task**\n",
    "    The folowing code is used to create a dataset for the copy task. The DNC receives in input batches of collections of string which have to be copied a number of times which is defined randomly.\n",
    "- **Training Module** \n",
    "    Finally the code for the trainig is porvided. This code allows also to save the variables of the model for checkpoint it and restore the values for an input evaluation. Note that the variables are saved and not the model, this mean that it's needed to reacreate the graph then loead the variables during a session.\n",
    "\n",
    "<br>\n",
    "Here follows the code license:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# ==========================================================================\n",
    "# Copyright 2017 Google Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==========================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "To performs some test on the model we had to modify the way the dataset was originted. The first test is to understand how much the model is sensitive to noised data. We have taken the original generation code and applied some modifications. For random betchs (where the randomness is further explained) we flipped the values of the bit in the string to copy alterning the target. That is, to deceive the DNC to perform it's original task of copying a string of bits several times we have changed the target. So we let the DNC assume it made it wrong and underdtand if for some samples this may cause a non convergence of the error to 0.\n",
    "<br>\n",
    "Before analysizing the noised version we have to test the DNC in a normal copy task with non altered target values. We can exploit the modified code in order to create the training environment over which manipulate the DNC training which allows for both non noised and noised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "import argparse\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _dnc\n",
    "import _repeat_copy_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Class\n",
    "Here we have createda a custom class, mainly a wrapper for the *train.py* script which allows to instantiate a class for the training and pass the initilization variables for the model and the training itself. For future test it's possible to directly override the train method and keep the interface with the DNC consistent. That is, it's only needed to add to the Tensorflow Graph operations which allows the training such as the computation of the loss and then run the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training():\n",
    "    \n",
    "    def __init__(self, hidden_size, memory_size, word_size, num_write_heads, num_read_heads, clip_value, \n",
    "                max_grad_norm, learning_rate, optimizier_epsy, batch_size, numb_bits, min_length, max_length, \n",
    "                min_repeats, max_repeats):\n",
    "        \n",
    "        self._hidden_size = hidden_size \n",
    "        self._memory_size = memory_size\n",
    "        self._word_size = word_size\n",
    "        self._num_write_heads = num_write_heads\n",
    "        self._num_read_heads = num_read_heads\n",
    "        self._clip_value = clip_value\n",
    "\n",
    "        self._max_grad_norm = max_grad_norm\n",
    "        self._learning_rate = learning_rate\n",
    "        self._optimizier_epsy = optimizier_epsy\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._numb_bits = numb_bits\n",
    "        self._min_length = min_length\n",
    "        self._max_length = max_length\n",
    "        self._min_repeats = min_repeats\n",
    "        self._max_repeats = max_repeats\n",
    "  \n",
    "\n",
    "    def run_model(self, input_sequence, output_size):\n",
    "        \"\"\"Runs model on input sequence.\"\"\"\n",
    "\n",
    "        access_config = {\n",
    "            \"memory_size\": self._memory_size,\n",
    "            \"word_size\": self._word_size,\n",
    "            \"num_reads\": self._num_read_heads,\n",
    "            \"num_writes\": self._num_write_heads,\n",
    "        }\n",
    "        controller_config = {\n",
    "            \"hidden_size\": self._hidden_size,\n",
    "        }\n",
    "        clip_value = self._clip_value\n",
    "        \n",
    "        dnc_core = _dnc.DNC(access_config, controller_config, output_size, clip_value)\n",
    "        initial_state = dnc_core.initial_state(self._batch_size)\n",
    "\n",
    "        output_sequence, _ = tf.nn.dynamic_rnn(\n",
    "            # instance of a RNN core module\n",
    "            cell=dnc_core,\n",
    "            inputs=input_sequence,\n",
    "            time_major=True,\n",
    "            initial_state=initial_state)\n",
    "\n",
    "        return output_sequence\n",
    "\n",
    "\n",
    "    def train(self, num_training_iterations, report_interval, checkpoint_dir='/tmp/tf/dnc', \n",
    "              checkpoint_interval=-1, with_distorsion=False, \n",
    "              with_noise=False, noise_level=None, verbosity=0):\n",
    "        \"\"\"Trains the DNC and periodically reports the loss.\"\"\"\n",
    "\n",
    "\n",
    "        #  dataset definition\n",
    "        dataset_noise = _repeat_copy_noise.NoisedRepeatCopy(self._numb_bits, self._batch_size,\n",
    "                                                           self._min_length, self._max_length,\n",
    "                                                           self._min_repeats, self._max_repeats,\n",
    "                                                           noise_level=noise_level)\n",
    "        \n",
    "        dataset_tensors = dataset_noise()\n",
    "\n",
    "        #  compute the output of the RNN\n",
    "        output_logits = self.run_model(dataset_tensors.observations, dataset_noise.target_size)\n",
    "        \n",
    "        # Used for visualization\n",
    "        output = tf.round(\n",
    "            tf.expand_dims(dataset_tensors.mask, -1) * tf.sigmoid(output_logits))\n",
    "\n",
    "        #  compute the loss with the respect of the target\n",
    "        train_loss = dataset_noise.cost(output_logits, dataset_tensors.target_noise,\n",
    "                                        dataset_tensors.mask)\n",
    "\n",
    "        # Set up optimizer with global norm clipping.\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "\n",
    "        #  compute the gradient\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(train_loss, trainable_variables), self._max_grad_norm)\n",
    "\n",
    "        # define a global variable\n",
    "        global_step = tf.get_variable(\n",
    "            name=\"global_step\",\n",
    "            shape=[],\n",
    "            dtype=tf.int64,\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            trainable=False,\n",
    "            collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n",
    "\n",
    "        #  run the backward gradient propagation\n",
    "        optimizer = tf.train.RMSPropOptimizer(\n",
    "            self._learning_rate, epsilon=self._optimizier_epsy)\n",
    "        \n",
    "        # GLOBAL -> this is incremented by one after the minimization (backward) have been executed\n",
    "        train_step = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables), global_step=global_step)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        if checkpoint_interval > 0:\n",
    "            hooks = [\n",
    "                tf.train.CheckpointSaverHook(\n",
    "                    checkpoint_dir=checkpoint_dir,\n",
    "                    save_steps=checkpoint_interval,\n",
    "                    saver=saver)\n",
    "            ]\n",
    "        else:\n",
    "            hooks = []\n",
    "        \n",
    "        # open the session\n",
    "        with tf.train.SingularMonitoredSession(\n",
    "                hooks=hooks, checkpoint_dir=checkpoint_dir) as sess:\n",
    "    \n",
    "            start_iteration = sess.run(global_step)\n",
    "            tf.logging.info(\" - Training has started!\")\n",
    "            \n",
    "            # save the result for further analysis\n",
    "            results = {\n",
    "                'noise_level': NOISE_LEVEL,\n",
    "                'iteration': [],\n",
    "                'outputs': [],\n",
    "                'data': [],\n",
    "                'strings': [],\n",
    "                'losses': []\n",
    "            }\n",
    "\n",
    "            total_loss = 0\n",
    "            for train_iteration in range(start_iteration, num_training_iterations):\n",
    "                _, loss = sess.run([train_step, train_loss])\n",
    "                total_loss += loss\n",
    "\n",
    "                if (train_iteration + 1) % report_interval == 0:\n",
    "                    dataset_tensors_np, output_np = sess.run([dataset_tensors, output])\n",
    "                    dataset_string = dataset_noise.to_human_readable(dataset_tensors_np, output_np,\n",
    "                                                                     with_distortion=with_distorsion,\n",
    "                                                                     with_noise=with_noise)\n",
    "\n",
    "                    results['iteration'].append(train_iteration)\n",
    "                    results['data'].append(dataset_tensors_np)\n",
    "                    results['strings'].append(dataset_string)\n",
    "                    results['outputs'].append(output_np)\n",
    "                    results['losses'].append(total_loss / report_interval)\n",
    "\n",
    "                    if verbosity is 0:\n",
    "                        tf.logging.info(\" - Iter: %d: Avg training loss %f.\",\n",
    "                                        train_iteration, total_loss / report_interval)\n",
    "                    else:\n",
    "                        tf.logging.info(\" - Iter: %d: Avg training loss %f.\\n%s\",\n",
    "                                        train_iteration, total_loss / report_interval,\n",
    "                                        dataset_string)\n",
    "                    total_loss = 0\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Noise Experiments\n",
    "Now we define the parameters for the model and for the task. We first run a non-noised version of the copy task, then we slightly include noise in the training set to see how this will affect the ability of the DNC on perform the copy right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "HIDDEN_SIZE = 64\n",
    "MEMORY_SIZE = 16\n",
    "WORD_SIZE = 16\n",
    "NUM_WRITE_HEADS = 1\n",
    "NUM_READ_HEADS = 4\n",
    "CLIP_VALUE = 20\n",
    "\n",
    "# Optimizer parameters.\n",
    "MAX_GRAD_NORM = 50\n",
    "LEARN_RATE = 1e-4\n",
    "OPTIMIZER_EPSY = 1e-10\n",
    "\n",
    "# Task parameters\n",
    "BATCH_SIZE = 16\n",
    "NUMB_BITS = 4\n",
    "MIN_LENGTH = 1\n",
    "MAX_LENGTH = 2\n",
    "MIN_REPEATS = 1\n",
    "MAX_REPEATS = 2\n",
    "\n",
    "# Training options.\n",
    "TRAINING_ITER = 1000\n",
    "REPORT_INTERV = 100\n",
    "CHECHK_POINT_DIR = \"/tmp/tf/dnc\"\n",
    "CHECK_POINT_INTERVAL = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 16, 5), dtype=float32)\n",
      "Tensor(\"repeat_copy/Reshape_50:0\", shape=(?, 16, 5), dtype=float32)\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow: - Training has started!\n",
      "INFO:tensorflow: - Iter: 4999: Avg training loss 7.938283.\n",
      "INFO:tensorflow: - Iter: 9999: Avg training loss 4.396687.\n",
      "INFO:tensorflow: - Iter: 14999: Avg training loss 2.364800.\n",
      "INFO:tensorflow: - Iter: 19999: Avg training loss 1.378604.\n",
      "INFO:tensorflow: - Iter: 24999: Avg training loss 0.773323.\n",
      "INFO:tensorflow: - Iter: 29999: Avg training loss 0.333952.\n",
      "INFO:tensorflow: - Iter: 34999: Avg training loss 0.185517.\n",
      "INFO:tensorflow: - Iter: 39999: Avg training loss 0.127265.\n",
      "INFO:tensorflow: - Iter: 44999: Avg training loss 0.096554.\n",
      "INFO:tensorflow: - Iter: 49999: Avg training loss 0.080684.\n",
      "INFO:tensorflow: - Iter: 54999: Avg training loss 0.060898.\n",
      "INFO:tensorflow: - Iter: 59999: Avg training loss 0.051502.\n",
      "INFO:tensorflow: - Iter: 64999: Avg training loss 0.051568.\n",
      "INFO:tensorflow: - Iter: 69999: Avg training loss 0.054861.\n",
      "INFO:tensorflow: - Iter: 74999: Avg training loss 0.048359.\n",
      "INFO:tensorflow: - Iter: 79999: Avg training loss 0.054208.\n",
      "INFO:tensorflow: - Iter: 84999: Avg training loss 0.049640.\n",
      "INFO:tensorflow: - Iter: 89999: Avg training loss 0.037666.\n",
      "INFO:tensorflow: - Iter: 94999: Avg training loss 0.040014.\n",
      "INFO:tensorflow: - Iter: 99999: Avg training loss 0.026952.\n"
     ]
    }
   ],
   "source": [
    "NOISE_LEVEL = None\n",
    "TRAINING_ITER = 100000\n",
    "REPORT_INTERV = 5000\n",
    "\n",
    "training = Training(HIDDEN_SIZE, MEMORY_SIZE, WORD_SIZE, NUM_WRITE_HEADS, NUM_READ_HEADS, \n",
    "                   CLIP_VALUE, MAX_GRAD_NORM, LEARN_RATE, OPTIMIZER_EPSY, BATCH_SIZE, \n",
    "                   NUMB_BITS, MIN_LENGTH, MAX_LENGTH, MIN_REPEATS, MAX_REPEATS)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.logging.set_verbosity(3)  # Print INFO log messages.\n",
    "results_clean = training.train(TRAINING_ITER, REPORT_INTERV, with_noise=False, with_distorsion=False, \n",
    "                               noise_level=NOISE_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 16, 5), dtype=float32)\n",
      "Tensor(\"repeat_copy/Reshape_50:0\", shape=(?, 16, 5), dtype=float32)\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow: - Training has started!\n",
      "INFO:tensorflow: - Iter: 4999: Avg training loss 8.309622.\n",
      "INFO:tensorflow: - Iter: 9999: Avg training loss 5.732491.\n",
      "INFO:tensorflow: - Iter: 14999: Avg training loss 3.877566.\n",
      "INFO:tensorflow: - Iter: 19999: Avg training loss 3.107155.\n",
      "INFO:tensorflow: - Iter: 24999: Avg training loss 2.822140.\n",
      "INFO:tensorflow: - Iter: 29999: Avg training loss 2.637930.\n",
      "INFO:tensorflow: - Iter: 34999: Avg training loss 2.483084.\n",
      "INFO:tensorflow: - Iter: 39999: Avg training loss 2.352814.\n",
      "INFO:tensorflow: - Iter: 44999: Avg training loss 2.306345.\n",
      "INFO:tensorflow: - Iter: 49999: Avg training loss 2.259139.\n",
      "INFO:tensorflow: - Iter: 54999: Avg training loss 2.233495.\n",
      "INFO:tensorflow: - Iter: 59999: Avg training loss 2.212310.\n",
      "INFO:tensorflow: - Iter: 64999: Avg training loss 2.206566.\n",
      "INFO:tensorflow: - Iter: 69999: Avg training loss 2.188561.\n",
      "INFO:tensorflow: - Iter: 74999: Avg training loss 2.205978.\n",
      "INFO:tensorflow: - Iter: 79999: Avg training loss 2.203298.\n",
      "INFO:tensorflow: - Iter: 84999: Avg training loss 2.184547.\n",
      "INFO:tensorflow: - Iter: 89999: Avg training loss 2.174551.\n",
      "INFO:tensorflow: - Iter: 94999: Avg training loss 2.171541.\n",
      "INFO:tensorflow: - Iter: 99999: Avg training loss 2.164614.\n"
     ]
    }
   ],
   "source": [
    "NOISE_LEVEL = 85.\n",
    "TRAINING_ITER = 100000\n",
    "REPORT_INTERV = 5000\n",
    "\n",
    "training = Training(HIDDEN_SIZE, MEMORY_SIZE, WORD_SIZE, NUM_WRITE_HEADS, NUM_READ_HEADS, \n",
    "                   CLIP_VALUE, MAX_GRAD_NORM, LEARN_RATE, OPTIMIZER_EPSY, BATCH_SIZE, \n",
    "                   NUMB_BITS, MIN_LENGTH, MAX_LENGTH, MIN_REPEATS, MAX_REPEATS)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.logging.set_verbosity(3)  # Print INFO log messages.\n",
    "results_noise = training.train(TRAINING_ITER, REPORT_INTERV, with_noise=False, with_distorsion=False, \n",
    "                               noise_level=NOISE_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This *matrix* shows the number of changed bits. It has to be red as follow: there is one row per iteration saved and one column for each batch. Take into account that for each iteration 16 batched are generated, in this case only one has been affected by noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for indx in range(len(results_noise['data'])):\n",
    "    print(results_noise['data'][indx][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "1. Migliorare il test e renderlo meno pesante visivamente.\n",
    "2. Definire una classe per dare un indice di performances (compare vettore di output con target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here instead we show the result for each saved iteration. It's possible to observe the observation, which is the input of the DNC and the model output with the respect of the target. To be noticed, the last two rows in the observation are the termination channel and the repetition channel respectively. For the model output the last row is the termination channel. Those channel are not part of the array to be copied but just extra information passed to the DNC conroller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Iteration: 4999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 8.309621811771393\n",
      ">>>> Iteration: 9999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      ">>>> Losses: 5.732491242980957\n",
      ">>>> Iteration: 14999\n",
      "\n",
      "Observations:\n",
      "+- 1 1 - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - - 1 - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - - 1 - -+\n",
      ">>>> Losses: 3.8775656994581222\n",
      ">>>> Iteration: 19999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 3.1071551090717318\n",
      ">>>> Iteration: 24999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 1 - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 2 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - 1 1 1 1 -+\n",
      "+- - - - - 1 - 1 -+\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - - - - - 1+\n",
      "\n",
      "Model Output:\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - 1 1 1 1 -+\n",
      "+- - - - - 1 - 1 -+\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - - - - - 1+\n",
      ">>>> Losses: 2.8221403005599974\n",
      ">>>> Iteration: 29999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 2.6379295472860336\n",
      ">>>> Iteration: 34999\n",
      "\n",
      "Observations:\n",
      "+- 1 1 - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 2 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - 1 1 1 1 -+\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - 1+\n",
      "\n",
      "Model Output:\n",
      "+- - - - 1 1 1 1 -+\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - 1+\n",
      ">>>> Losses: 2.483083514547348\n",
      ">>>> Iteration: 39999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 2.352814322423935\n",
      ">>>> Iteration: 44999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 2 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - 1 -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - 1+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - 1 - 1 -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - 1+\n",
      ">>>> Losses: 2.3063452721595765\n",
      ">>>> Iteration: 49999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      ">>>> Losses: 2.2591393342256545\n",
      ">>>> Iteration: 54999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 1 - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 2 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - 1 1 1 1 -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - 1+\n",
      "\n",
      "Model Output:\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - 1 - 1 - -+\n",
      "+- - - - 1 1 1 1 -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - 1+\n",
      ">>>> Losses: 2.233495108151436\n",
      ">>>> Iteration: 59999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 2.2123097828388216\n",
      ">>>> Iteration: 64999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - -+\n",
      "+- - - - - - -+\n",
      "+- - - - - - -+\n",
      "+- 1 - - - - -+\n",
      "+1 - - - - - -+\n",
      "+- - 1 - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - 1 - - -+\n",
      "+- - - - - - -+\n",
      "+- - - - - - -+\n",
      "+- - - 1 - - -+\n",
      "+- - - - 1 - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - 1 - - -+\n",
      "+- - - - - - -+\n",
      "+- - - - - - -+\n",
      "+- - - 1 - - -+\n",
      "+- - - - 1 - -+\n",
      ">>>> Losses: 2.206565974211693\n",
      ">>>> Iteration: 69999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 2.1885611533641813\n",
      ">>>> Iteration: 74999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 2.205977590751648\n",
      ">>>> Iteration: 79999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "+- 1 1 - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - - 1 - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - - 1 - -+\n",
      ">>>> Losses: 2.203298367023468\n",
      ">>>> Iteration: 84999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "+- - - - 1 - - - -+\n",
      ">>>> Losses: 2.184546591448784\n",
      ">>>> Iteration: 89999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 1 - - - - - -+\n",
      "+- - 1 - - - - - -+\n",
      "+- 1 1 - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - - 1 - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - - - 1 - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - - 1 - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - - - 1 - -+\n",
      ">>>> Losses: 2.1745511697769166\n",
      ">>>> Iteration: 94999\n",
      "\n",
      "Observations:\n",
      "+- - - - - - - - -+\n",
      "+- 1 1 - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - - 1 - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - - - - - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - 1 - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - - - - - - -+\n",
      "+- - - - 1 1 - - -+\n",
      "+- - - - 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - - - - 1 - -+\n",
      ">>>> Losses: 2.171540751314163\n",
      ">>>> Iteration: 99999\n",
      "\n",
      "Observations:\n",
      "+- 1 - - - - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+- 1 - - - - - - -+\n",
      "+1 - - - - - - - -+\n",
      "+- - 2 - - - - - -+\n",
      "\n",
      "Targs:\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      "\n",
      "Model Output:\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - 1 1 - - - -+\n",
      "+- - - - - 1 - - -+\n",
      ">>>> Losses: 2.164613728427887\n"
     ]
    }
   ],
   "source": [
    "for string, iteration, loss in zip(results_noise['strings'], \n",
    "                                   results_noise['iteration'],\n",
    "                                   results_noise['losses']):\n",
    "    print('\\n>>>> Iteration: ' + str(iteration))\n",
    "    print(string)\n",
    "    print('>>>> Losses: '+ str(loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "It's possible to see how the noised version is slower in converge to a zero error. We should look for longer training iterations in order to see how much it will improve and if this has some noticeble effects on the copy task. Indeed, for now, even in the case of some noise, the DNC was able to copy the vector without errors in the observed iterations. Further, we should investigate different velues of the noise and different parameter of the task (like increasing the size of the array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Task\n",
    "Now we propose an implementation for the **Graph Tasks** as described in the paper. \n",
    "Before proceding with the code we provided some implementation design choices for the tasks.\n",
    "<br>\n",
    "\n",
    "### Graph Task Description\n",
    "The idea is to perform the following tasks on a graph:\n",
    "- Traversal \n",
    "- Shortest Path\n",
    "- Inference\n",
    "\n",
    "All tasks need a random generation of graph triples consisting of a *source label*, *edge label*, *destination label*. To achieve so, we first generate a random graph by sampling the unit square *N* nodes. Then, *N* numbers are drawn uniformly between $0$ and $999$ and assign randomly to the nodes. This makes the node labels to be unique across the nodes of the network. To decide the neighbors of each node, the KNN (K-nearest neighbour) is used. Once selected the possible neihbours, edges are created between the nodes. Each edge for the neigbourhood is given a label which is a random number among the $N$ already selected for the nodes. For the **Traversal** and the **Shortest Path**, the *edge label* is unique in the neighborhood but not unique in the graph. This means that the DNC has to infer the possible unique relation between two given nodes and their relative edge.\n",
    "\n",
    "#### Encoding\n",
    "In order to provide those labels to the DNC controller, the triples are encode using a **10-way one-hot encoding**. That is, each digit has been encoded using a $10$ bit vector with all zeros except the position which identifies the digit set to $1$. This is done for all the $9$ digit representing a triple creating a **90 bit** vector. There is a **special reserved character** (\\_) which indicates that there is no input for the given label, this is represented with the zero vector with the same lengts as if there were a digit.\n",
    "<br>\n",
    "\n",
    "#### Phases\n",
    "For the training, the DNC controller receives sequences, each one representing a triple. The seuqences are divided into four phases:\n",
    "1. **Descritpion Phase**: used to present to the NN the triples of the graph over which perform the tasks.\n",
    "2. **Query Phase**: used to present queries to the NN based on the proposed graph.\n",
    "3. **Planning Phase**: no input are provided here, the phase is used to allow the DNC controller to prepare the answers.\n",
    "4. **Answer Phase**: a part for the *Shortest Path Task*, no input is presented here. During this phase the DNC outputs the answers for the given queries. In this fase, and only here, also the targets are used to compute the loss and train the network.\n",
    "\n",
    "In order to distinguish the different phases and indicates a transition, there are two additional bits, called channels, included in the input. The design choice is to let the bit of the first channel to be 1 only when there is a trainsition between the phases, and the bit of the second channel is only set to $1$ during all the **Answer Phase**.\n",
    "\n",
    "#### Output and Loss\n",
    "The output of the DNC controller are 90 bits, representing nine softmax distribution over the 10 bits. The loss for given sequences is computed as the sum of the log-probabilities of correctly predicting the bits for the nine digits.\n",
    "<br>\n",
    "*Here we have to comment about a possibile error in the paper where the sum is computed from 0 to 9. We have to argue due to this in not complient with what is written before the formula.*\n",
    "<br>\n",
    "Given input sequences $\\mathbf{x}$ and target $\\mathbf{z}$, the loss is computed as follow:\n",
    "<br>\n",
    "<br>\n",
    "    $\\mathcal{L}(\\mathbf{x},\\mathbf{z})=\\sum_{t=1}^T \\{ A(t) \\sum_{d=0}^8 log[Pr(\\mathbf{z_{t}^d}|\\mathbf{y_{t}^d}) ] \\} $\n",
    "<br>\n",
    "\n",
    "where $\\mathbf{z_{t}^d}$ is the target for the sequence $t$ and for the digit $d$, $\\mathbf{y_{t}^d}$ is the output fo the DNC controller for the sequence $t$ and for the digit $d$, $A(t)$ is a binary value whcih indicates when the loss has to be consider valid for the trainign. That is, ths value is $1$ during the **Answer Phase** and $0$ in the other ones. For the implementation we have created a mask for simulating this scenario, the mask entry is set to 1 for those seuqences for whcih the training is required.\n",
    "\n",
    "The **Network Prediction** is determined as the mode of the output of the single softmax distribution over the 10 bits for each of the nine digits. When the network has finished to output an answer, a special output pattern is given as idicated in the paper. Our choise, as non further detail are provided in the paper, is to output all (\\_) characters which means a $90$ bit zero vector as termination pattern. The DNC has been trained also on this special output. \n",
    "\n",
    "Finally, as a performance metric, the total right predictions over all possibile ones is given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traversal Task\n",
    "The first task we implement is the Traversal one. This is based on random walks generated from a random Graph. As we described before, during the **Description Phase** the graph triples are presented to the NN, then a series of Queries and Answer tripels. During the **Query Phase**, the first triple is made of the *source node* and *the edge* label (source label, edge label, \\_) with the destination missing. The further triples contains only the *edge label* (\\_, edge label, \\_). The DNC has to save the save the triples of the graph and infer the right nodes which from a source node connects all the nodes of the random walks. We reming that the *edge labels* are not unique accross the graph. That is, the network has to remember the previous destination node of a triple and use it as source label for the following triple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities\n",
    "Here we show the functions which are to define particular data manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ten_way_one_hot_encoder(number, dim=3):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    if number is not None:\n",
    "        encoded = []\n",
    "        for digit in str(number).zfill(3):\n",
    "            zeros = [0] * 10\n",
    "            zeros[int(digit)] = 1\n",
    "            encoded += zeros\n",
    "        return np.array(encoded, dtype=np.int8)\n",
    "    else:\n",
    "        return np.zeros(10 * dim, dtype=np.int8)\n",
    "\n",
    "\n",
    "def _ten_way_one_hot_decoder(vector, dim=3):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    # identify the special vector\n",
    "    if np.max(vector) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        decoded = ''\n",
    "        for label_b in vector.reshape((dim, 10)):\n",
    "            number_vector = np.array(range(0, 10))\n",
    "            value = number_vector.dot(label_b)\n",
    "            decoded += str(np.max(value))\n",
    "        return int(decoded)\n",
    "\n",
    "\n",
    "def _compute_prediction(triple_distribution):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    mode_triples = np.argmax(triple_distribution.reshape(9, 10), axis=1).reshape(3, 3)\n",
    "    labels = []\n",
    "    for mode_triple in mode_triples:\n",
    "        for mode in mode_triple:\n",
    "            zeros = [0] * 10\n",
    "            zeros[mode] = 1\n",
    "            labels += zeros\n",
    "    return np.array(labels)\n",
    "\n",
    "\n",
    "def _compare_triples(output_logit, target):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    triple_output = _compute_prediction(output_logit)\n",
    "    return np.array_equal(triple_output, target)\n",
    "\n",
    "\n",
    "def _human_readable_triple(triple):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    # treat the None and convert to a string\n",
    "    restore_triple = []\n",
    "    for label in triple:\n",
    "        if label is None:\n",
    "            restore_triple.append('_')\n",
    "        else:\n",
    "            restore_triple.append(str(label))\n",
    "\n",
    "    return \"({}, {}, {})\".format(*restore_triple)\n",
    "\n",
    "\n",
    "def encode_triple(triple):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    encoded = []\n",
    "    for label in triple:\n",
    "        encoded.append(_ten_way_one_hot_encoder(label))\n",
    "    return tuple(encoded)\n",
    "\n",
    "\n",
    "def decode_triple(triple):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    decoded = []\n",
    "    for label in triple:\n",
    "        decoded.append(_ten_way_one_hot_decoder(label))\n",
    "    return tuple(decoded)\n",
    "\n",
    "\n",
    "def single_array_triple(triple):\n",
    "    return np.stack(triple).flatten()\n",
    "\n",
    "\n",
    "def multiple_array_triple(triple, dim=3):\n",
    "    return np.split(triple, dim)\n",
    "\n",
    "\n",
    "def evaluate_prediction(outputs, masks, targets):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    overall_predicted = []\n",
    "    # retrieve the size of the batch and the sequences from the output shape\n",
    "    batch_size = outputs.shape[1]\n",
    "    sequences = outputs.shape[0]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        predicted = 0\n",
    "        total = 0\n",
    "        for k in range(sequences):\n",
    "            if (masks[k, i] != 0) and np.sum(targets[k, i, :] > 0):\n",
    "                total += 1\n",
    "                if _compare_triples(outputs[k, i, :], targets[k, i, :]):\n",
    "                    predicted += 1\n",
    "        overall_predicted.append(predicted / total)\n",
    "\n",
    "    return np.mean(np.array(overall_predicted))\n",
    "\n",
    "\n",
    "def decode_predictions_with_queries(outputs, observations, targets, masks, query_start_index, batch_index=0):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    # isolate the mask for the batch and the queries and answers sequences only\n",
    "    masks_ = masks[query_start_index:, batch_index]\n",
    "    outputs_ = outputs[query_start_index:, batch_index, :]\n",
    "    targets_ = targets[query_start_index:, batch_index, :]\n",
    "    observations_ = observations[query_start_index:, batch_index, :90]\n",
    "\n",
    "    final_queries_answers = []\n",
    "\n",
    "    queries_list = []\n",
    "    answers_list = []\n",
    "    targets_list = []\n",
    "    for i in range(len(masks_)):\n",
    "\n",
    "        # if the mask is 0 is a query and they are contiguous\n",
    "        if masks_[i] == 0:\n",
    "            encoded_vector = multiple_array_triple(observations_[i, :])\n",
    "            decoded_triple = decode_triple(encoded_vector)\n",
    "            queries_list.append(decoded_triple)\n",
    "\n",
    "        # if the mask is 1 and there are less answers than queries\n",
    "        if masks_[i] == 1 and len(queries_list) != len(answers_list):\n",
    "            # first retrieve the predictions\n",
    "            predictions = _compute_prediction(outputs_[i, :])\n",
    "            # use the decoder to retrieve the triple\n",
    "            encoded_vector = multiple_array_triple(predictions)\n",
    "            decoded_triple = decode_triple(encoded_vector)\n",
    "            answers_list.append(decoded_triple)\n",
    "            # do the same for the targets\n",
    "            encoded_vector = multiple_array_triple(targets_[i, :])\n",
    "            decoded_triple = decode_triple(encoded_vector)\n",
    "            targets_list.append(decoded_triple)\n",
    "        # else, it's the 1 for the termination pattern, skip this\n",
    "        elif len(queries_list) == len(answers_list):\n",
    "            # save the prediction\n",
    "            final_queries_answers.append((queries_list, answers_list, targets_list))\n",
    "            # reset the vectors\n",
    "            queries_list = []\n",
    "            answers_list = []\n",
    "            targets_list = []\n",
    "\n",
    "    return final_queries_answers\n",
    "\n",
    "\n",
    "def query_answer_human_readable(queries_answers, max_numb_qa=5):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    if max_numb_qa > len(queries_answers) or max_numb_qa == \"max\":\n",
    "        max_numb_qa = len(queries_answers)\n",
    "\n",
    "    queries_answers_strings = []\n",
    "\n",
    "    for i in range(max_numb_qa):\n",
    "        string = \"\"\n",
    "        queries, answers, targets = queries_answers[i][0], queries_answers[i][1], queries_answers[i][2]\n",
    "\n",
    "        string += \"#### QA {} ####\\n\".format(str(i))\n",
    "        string += \">>> Queries:\\n\"\n",
    "        for q in queries:\n",
    "            string += _human_readable_triple(q) + '\\n'\n",
    "\n",
    "        string += \">>> Answers:\\n\"\n",
    "        for a in answers:\n",
    "            string += _human_readable_triple(a) + '\\n'\n",
    "\n",
    "        string += \">>> Targets:\\n\"\n",
    "        for t in targets:\n",
    "            string += _human_readable_triple(t) + '\\n'\n",
    "        string += '\\n'\n",
    "\n",
    "        # append the string\n",
    "        queries_answers_strings.append(string)\n",
    "\n",
    "    return queries_answers_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_sigmoid_cross_entropy(logits, target, mask):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    # compute the cross entropy given the output of the neural network and the targets\n",
    "    # this implements the loss described above\n",
    "    xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=logits)\n",
    "    # sum over the digits (this is the sum over the whole 90 bit-long vector)\n",
    "    loss_time_batch = tf.reduce_sum(xent, axis=2)\n",
    "    # sum over the sequences applying a mask to hide the sequences not related with the target\n",
    "    loss_batch = tf.reduce_sum(loss_time_batch * mask, axis=0)\n",
    "    # get the batch size using the dimensions of the logits\n",
    "    batch_size = tf.cast(tf.shape(logits)[1], dtype=loss_time_batch.dtype)\n",
    "    # compute the loss for the total batch\n",
    "    loss = tf.reduce_sum(loss_batch) / batch_size\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Generation\n",
    "The class has been created to reflect the graph creation as described in the paper.\n",
    "We used the *Networkx* libarary to have a structure to define nodes and edges, which allows for a fast look-up of neighbours and edges for a given node. The class allows also to define the random walks on the graph passing the deepness of the walks. Not all walks are guaranteed to reach the specified deepness, so a length parameter is used to select path all of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Class used to define the graph for the DNC Graph task\"\"\"\n",
    "\n",
    "class GraphGen(object):\n",
    "    \"\"\"Class usedto generate a Graph. It supplies also methods for drawing random walks over the graph.\"\"\"\n",
    "    def __init__(self, low_knn_bound, upper_knn_bound, numb_nodes, save_folder=None):\n",
    "        self._low_knn_bound = low_knn_bound\n",
    "        self._upper_knn_bound = upper_knn_bound\n",
    "        self._numb_nodes = numb_nodes\n",
    "        self._save_folder = save_folder\n",
    "        self._node_mapping = {}\n",
    "        self._triples = []\n",
    "        self._random_walks = []\n",
    "        self._graph = None\n",
    "\n",
    "    def _get_node(self, coordinate):\n",
    "        return self._node_mapping[tuple(coordinate)]\n",
    "\n",
    "    def _edge_knn(self, nodes, labels):\n",
    "        # define the NN class\n",
    "        nn = NearestNeighbors()\n",
    "        nn.fit(nodes)\n",
    "\n",
    "        for node in self._graph.nodes():\n",
    "            # draw a number for the k parameter from an uniform distribution\n",
    "            k = np.random.randint(self._low_knn_bound, self._upper_knn_bound)\n",
    "            # compute the k nearest neighbor\n",
    "            coordinates = np.array([self._graph.node[node]['x_coordinates'],\n",
    "                                    self._graph.node[node]['y_coordinates']])\n",
    "            _, index = nn.kneighbors(coordinates.reshape(1, 2), n_neighbors=k + 1)\n",
    "            # given the index we retrieve the coordinates, exclude itself from the list\n",
    "            # the labels for the edge are not unique across the graph and are drawn from the label\n",
    "            # previously selected for the nodes\n",
    "            edge_labels = np.random.choice(labels, size=k, replace=False)\n",
    "            for neighbor_index, edge_label in zip(index[0, 1:], edge_labels):\n",
    "                # crete the edge between the current node and the neighbor\n",
    "                # retrieve the given label for that node\n",
    "                neighbor = self._get_node(nodes[neighbor_index])\n",
    "                # create the edge, update the Graph and create the triple\n",
    "                self._graph.add_edge(int(node), int(neighbor), weigth=1.0, label=int(edge_label))\n",
    "                self._triples.append((node, edge_label, neighbor))\n",
    "\n",
    "    def _seek_random_walks(self, node, depth=5):\n",
    "        triples = []\n",
    "        node_visited = [node]\n",
    "        curr_node = node\n",
    "        while depth:\n",
    "            # find the neighbours of the node and then shuffle the list\n",
    "            neighbors = nx.neighbors(self._graph, curr_node)\n",
    "            random.shuffle(neighbors)\n",
    "            current_neighbor = None\n",
    "            # look for a valid neighbor which does not create a loop\n",
    "            for neighbor in neighbors:\n",
    "                if neighbor not in node_visited:\n",
    "                    current_neighbor = neighbor\n",
    "                    break\n",
    "            # check if there is a neighbor which is valid\n",
    "            if current_neighbor is not None:\n",
    "                edge_label = self._graph[curr_node][current_neighbor]['label']\n",
    "                # generate the triple\n",
    "                triples.append((curr_node, edge_label, current_neighbor))\n",
    "                node_visited.append(current_neighbor)\n",
    "                # update the current node as the neighbor of the previous one\n",
    "                curr_node = current_neighbor\n",
    "                # decrement the depth of the walk\n",
    "            depth -= 1\n",
    "        return triples\n",
    "\n",
    "    def save_graph(self, name):\n",
    "        if self._save_folder is None:\n",
    "            raise self._NoSaveFolderException\n",
    "\n",
    "        nx.write_graphml(self._graph, self._save_folder + '/' + name + \".txt\")\n",
    "\n",
    "    def load_graph(self, name):\n",
    "        if self._save_folder is None:\n",
    "            raise self._NoSaveFolderException\n",
    "\n",
    "        self._graph = nx.read_graphml(self._save_folder + '/' + name + \".txt\")\n",
    "\n",
    "    def draw_graph(self):\n",
    "        # define an instance of graph\n",
    "        self._graph = nx.Graph()\n",
    "        # from the unit square draw uniformly 2-d points\n",
    "        nodes_sampled = np.random.uniform(size=(self._numb_nodes, 2))\n",
    "\n",
    "        # for each node, draw a integer form 1 ro 999 as label for the node\n",
    "        # the label must be unique, in case of an already drawn label,\n",
    "        node_labels = np.random.choice(range(1, 999), size=self._numb_nodes, replace=False)\n",
    "\n",
    "        for index, node in enumerate(nodes_sampled):\n",
    "            # add the mapping between label and coordinate\n",
    "            self._node_mapping[tuple(node)] = node_labels[index]\n",
    "            # add the node to the graph with the same id as the label\n",
    "            self._graph.add_node(int(node_labels[index]), x_coordinates=float(node[0]),\n",
    "                                 y_coordinates=float(node[1]))\n",
    "\n",
    "        self._edge_knn(nodes_sampled, node_labels)\n",
    "\n",
    "    def gen_random_walks(self, length=5, depth=5):\n",
    "        if length > depth:\n",
    "            raise self._ParameterException\n",
    "\n",
    "        if self._graph is None:\n",
    "            raise self._GraphGenerationException\n",
    "\n",
    "        # reset the walks\n",
    "        self._random_walks = []\n",
    "        for n in self._graph.nodes():\n",
    "            walk = self._seek_random_walks(n, depth=depth)\n",
    "            if len(walk) == length:\n",
    "                self._random_walks.append(walk)\n",
    "\n",
    "    def show_graph(self):\n",
    "        plt.figure()\n",
    "        nx.draw(self._graph, with_labels=True)\n",
    "        plt.show()\n",
    "\n",
    "    def shuffle_walks(self):\n",
    "        random.shuffle(self._random_walks)\n",
    "\n",
    "    @property\n",
    "    def get_triples(self):\n",
    "        return self._triples\n",
    "\n",
    "    @property\n",
    "    def get_graph(self):\n",
    "        return self._graph\n",
    "\n",
    "    @property\n",
    "    def get_random_walks(self):\n",
    "        return self._random_walks\n",
    "\n",
    "    class _GraphGenerationException(Exception):\n",
    "        @staticmethod\n",
    "        def print_message():\n",
    "            print(\"Need to generate the Graph before running the random walk!\")\n",
    "\n",
    "    class _ParameterException(Exception):\n",
    "        @staticmethod\n",
    "        def print_message():\n",
    "            print(\"The length of the walk should be <= the deepness of the generated random walks\")\n",
    "\n",
    "    class _NoSaveFolderException(Exception):\n",
    "        @staticmethod\n",
    "        def print_message():\n",
    "            print(\"The folder for saving the graph is not defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Traversal Task\n",
    "The class is used to generate a dataset for the Traversal Task. The idea is to generate a tensor of dimension (seuqence_size, batch_size, input_size) as input of the DNC controller. In order to be complient with the DNC definition as provided in the GitHub repository, the tansor is a **time major** one which is used to allow faster computation in run time. A time major tansor has the dimension of the sequences as first axis and the batch one as second axis. \n",
    "\n",
    "By default, $32$ random walks are generated, but in case of less random walks possible for the given graph the maximum possible number is used to generate the walks. It's possible to ask for a single or multiple dataset to be creared, in the case of a single one a dictionary is given instrad of a list of dictionaries. For each dataset, the observation (comprasing sequences for all the phases of the task) and targets are given. In addition a mask matrix (sequence_size, batch_size) is given to compute the loss as described above.\n",
    "\n",
    "For the case when no input are required to the DNC controller, we give a vector of zeros representing the special character describe before. This is also a design choice complient with the fact that the *no input* is described indeed as the zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Class used to generate the Traversal task's dataset.\"\"\"\n",
    "\n",
    "class DatasetTraversalGenerator(object):\n",
    "    def __init__(self, lower_bound, upper_bound, nodes, walk_length, walk_deepness, graph_folder, batch_size=16):\n",
    "        self._lower_bound = lower_bound\n",
    "        self._upper_bound = upper_bound\n",
    "        self._nodes = nodes\n",
    "        self._walk_length = walk_length\n",
    "        self._walk_deepness = walk_deepness\n",
    "        self._genGraph = GraphGen(lower_bound, upper_bound, nodes, save_folder=graph_folder)\n",
    "        self._genGraph.draw_graph()\n",
    "        self._genGraph.gen_random_walks(walk_length, walk_deepness)\n",
    "        self._batch_size = batch_size\n",
    "        self._query_index_start = None\n",
    "\n",
    "    def _generate_graph_description(self):\n",
    "        batch_input_description = []\n",
    "        for triple in self._genGraph.get_triples:\n",
    "            # add the channels (ch1 and ch2)\n",
    "            encoded_triple = np.concatenate(\n",
    "                (single_array_triple(encode_triple(triple)), np.array([0, 0], dtype=np.int8))\n",
    "            )\n",
    "            batch_input_description.append(encoded_triple)\n",
    "        # generate also the target for the batch (90 zeros and 1 for the mask)\n",
    "        batch_target_description = np.zeros((len(batch_input_description), 91), dtype=np.int8)\n",
    "        batch_input_description = np.array(batch_input_description)\n",
    "        return batch_input_description, batch_target_description\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_query_answer(walk, termination=True):\n",
    "        queries_walk_input = []\n",
    "        queries_walk_target = []\n",
    "        answers_walk_input = []\n",
    "        answers_walk_target = []\n",
    "\n",
    "        # the first triple of the walk has to include the source and the label\n",
    "        first = (walk[0][0], walk[0][1], None)\n",
    "\n",
    "        # the encoded triple has to include the channel 1 only for the first entry\n",
    "        first_encoded_query = np.concatenate(\n",
    "            (single_array_triple(encode_triple(first)), np.array([1, 0], dtype=np.int8))\n",
    "        )\n",
    "        # generate the corresponding answer (target) for the query and add the mask\n",
    "        first_encoded_answer = np.concatenate(\n",
    "            (single_array_triple(encode_triple(walk[0])), np.array([1], dtype=np.int8))\n",
    "        )\n",
    "\n",
    "        queries_walk_input.append(first_encoded_query)\n",
    "        # generate a null target with zeros\n",
    "        queries_walk_target.append(np.zeros(91, dtype=np.int8))\n",
    "\n",
    "        # include the two channels for the input and the transition\n",
    "        answers_walk_input.append(np.concatenate((np.zeros(90, dtype=np.int8), np.array([1, 1], dtype=np.int8))))\n",
    "        answers_walk_target.append(first_encoded_answer)\n",
    "\n",
    "        for triple in walk[1:]:\n",
    "            # the following triples has to include only the label\n",
    "            triples_i = (None, triple[1], None)\n",
    "            encoded_query = np.concatenate(\n",
    "                (single_array_triple(encode_triple(triples_i)), np.array([0, 0], dtype=np.int8))\n",
    "            )\n",
    "            encoded_answer = np.concatenate(\n",
    "                (single_array_triple(encode_triple(triple)), np.array([1], dtype=np.int8))\n",
    "            )\n",
    "\n",
    "            queries_walk_input.append(encoded_query)\n",
    "            queries_walk_target.append(np.zeros(91, dtype=np.int8))\n",
    "            # include the fake input for the answer phase with the channel set to 1\n",
    "            answers_walk_input.append(\n",
    "                np.concatenate(\n",
    "                    (np.zeros(90, dtype=np.int8), np.array([0, 1], dtype=np.int8))\n",
    "                )\n",
    "            )\n",
    "            answers_walk_target.append(encoded_answer)\n",
    "\n",
    "        if termination:\n",
    "            # need to include the termination target and input\n",
    "            answers_walk_input.append(\n",
    "                np.concatenate(\n",
    "                    (np.zeros(90, dtype=np.int8), np.array([0, 1], dtype=np.int8))\n",
    "                )\n",
    "            )\n",
    "            answers_walk_target.append(\n",
    "                np.concatenate(\n",
    "                    (np.zeros(90, dtype=np.int8), np.array([1], dtype=np.int8))\n",
    "                )\n",
    "            )\n",
    "\n",
    "        inputs = np.concatenate(\n",
    "            (np.array(queries_walk_input), np.array(answers_walk_input))\n",
    "        )\n",
    "        targets = np.concatenate(\n",
    "            (np.array(queries_walk_target), np.array(answers_walk_target))\n",
    "        )\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "    def _regenerate_graph(self):\n",
    "        self._genGraph = GraphGen(self._lower_bound, self._upper_bound, self._nodes)\n",
    "        self._genGraph.draw_graph()\n",
    "        self._genGraph.gen_random_walks(self._walk_length, self._walk_deepness)\n",
    "\n",
    "    def _generate_series_query_answer(self, size=32):\n",
    "        # select 32 random walks\n",
    "        if len(self._genGraph.get_random_walks) < size:\n",
    "            size = len(self._genGraph.get_random_walks)\n",
    "\n",
    "        # shuffle the vector of walks in order to present them in different order to the DNC\n",
    "        self._genGraph.shuffle_walks()\n",
    "        walks = self._genGraph.get_random_walks[0:size]\n",
    "        batch_input_qa, batch_target_qa = self._generate_query_answer(walks[0])\n",
    "\n",
    "        for walk in walks[1:]:\n",
    "            input, target = self._generate_query_answer(walk)\n",
    "            batch_input_qa = np.concatenate((batch_input_qa, input))\n",
    "            batch_target_qa = np.concatenate((batch_target_qa, target))\n",
    "\n",
    "        return batch_input_qa, batch_target_qa\n",
    "\n",
    "    def _build_batches(self):\n",
    "        observations = []\n",
    "        targets = []\n",
    "        masks = []\n",
    "        # this can be computed only once and then shuffled because the graph does not change across\n",
    "        # batches\n",
    "        input_description, target_description = self._generate_graph_description()\n",
    "        # save the length of the description sequences\n",
    "        self._query_index_start = len(input_description)\n",
    "\n",
    "        for _ in range(self._batch_size):\n",
    "            # shuffle the input description (the target does not change for the description phase)\n",
    "            np.random.shuffle(input_description)\n",
    "            # this have to be computed at each batch generation\n",
    "            input_qa, target_qa = self._generate_series_query_answer()\n",
    "            # retrieve the mask from the targets of both description and QA and concatenate\n",
    "            mask_description_batch = target_description[:, -1]\n",
    "            mask_qa_batch = target_qa[:, -1]\n",
    "            masks.append(np.concatenate((mask_description_batch, mask_qa_batch)))\n",
    "            # isolate only the targets of both description and QA and concatenate\n",
    "            target_description_batch = target_description[:, :-1]\n",
    "            target_qa_batch = target_qa[:, :-1]\n",
    "            targets.append(np.concatenate((target_description_batch, target_qa_batch)))\n",
    "            # concatenate the inputs\n",
    "            observations.append(np.concatenate((input_description, input_qa)))\n",
    "\n",
    "        # reshape in order to have a time-major tensors\n",
    "        obs, targets, masks = np.array(observations), np.array(targets), np.array(masks)\n",
    "\n",
    "        obs_ = np.moveaxis(obs, 0, 1)\n",
    "        targets_ = np.moveaxis(targets, 0, 1)\n",
    "        masks_ = np.moveaxis(masks, 0, 1)\n",
    "\n",
    "        return obs_, targets_, masks_\n",
    "\n",
    "    def generate_batches(self, size=10000):\n",
    "        datasets_collection = []\n",
    "        for _ in range(size):\n",
    "            dataset = {}\n",
    "            dataset['observations'], dataset['targets'], dataset['masks'] = self._build_batches()\n",
    "            datasets_collection.append(dataset)\n",
    "            if size == 1:\n",
    "                return dataset\n",
    "            self._regenerate_graph()\n",
    "        return datasets_collection\n",
    "\n",
    "    def save_graph_dataset(self, name=\"graph\"):\n",
    "        self._genGraph.save_graph(name)\n",
    "\n",
    "    def load_graph_dataset(self, name=\"graph\"):\n",
    "        self._genGraph.load_graph(name)\n",
    "\n",
    "    @property\n",
    "    def query_index_start(self):\n",
    "        return self._query_index_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Traversal\n",
    "The class used for defining the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingTraversal(object):\n",
    "    def __init__(self, hidden_size, memory_size, word_size, num_write_heads, num_read_heads, clip_value,\n",
    "                 model_output, max_grad_norm, learning_rate, optimizier_epsy, batch_size, lower_bound, \n",
    "                 upper_bound, nodes, walk_length, walk_deepness):\n",
    "        \"\"\"TODO\"\"\"\n",
    "        # model parameters\n",
    "        self._hidden_size = hidden_size \n",
    "        self._memory_size = memory_size\n",
    "        self._word_size = word_size\n",
    "        self._num_write_heads = num_write_heads\n",
    "        self._num_read_heads = num_read_heads\n",
    "        self._clip_value = clip_value\n",
    "        self._model_output = model_output\n",
    "        # training parameters\n",
    "        self._max_grad_norm = max_grad_norm\n",
    "        self._learning_rate = learning_rate\n",
    "        self._optimizier_epsy = optimizier_epsy\n",
    "        # task parameters\n",
    "        self._batch_size = batch_size\n",
    "        self._lower_bound = lower_bound\n",
    "        self._upper_bound = upper_bound\n",
    "        self._nodes = nodes\n",
    "        self._walk_length = walk_length\n",
    "        self._walk_deepness = walk_deepness\n",
    "\n",
    "\n",
    "    def masked_sigmoid_cross_entropy(logits, target, mask):\n",
    "        \"\"\"TODO\"\"\"\n",
    "        # compute the cross entropy given the output of the neural network and the targets\n",
    "        # this implements the loss described above\n",
    "        xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=logits)\n",
    "        # sum over the digits (this is the sum over the whole 90 bit-long vector)\n",
    "        loss_time_batch = tf.reduce_sum(xent, axis=2)\n",
    "        # sum over the sequences applying a mask to hide the sequences not related with the target\n",
    "        loss_batch = tf.reduce_sum(loss_time_batch * mask, axis=0)\n",
    "        # get the batch size using the dimensions of the logits\n",
    "        batch_size = tf.cast(tf.shape(logits)[1], dtype=loss_time_batch.dtype)\n",
    "        # compute the loss for the total batch\n",
    "        loss = tf.reduce_sum(loss_batch) / batch_size\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def run_model(self, input_sequence, output_size):\n",
    "        \"\"\"Runs model on input sequence.\"\"\"\n",
    "\n",
    "        access_config = {\n",
    "            \"memory_size\": self._memory_size,\n",
    "            \"word_size\": self._word_size,\n",
    "            \"num_reads\": self._num_read_heads,\n",
    "            \"num_writes\": self._num_write_heads,\n",
    "        }\n",
    "        controller_config = {\n",
    "            \"hidden_size\": self._hidden_size,\n",
    "        }\n",
    "        clip_value = self._clip_value\n",
    "        \n",
    "        dnc_core = _dnc.DNC(access_config, controller_config, output_size, clip_value)\n",
    "        initial_state = dnc_core.initial_state(self._batch_size)\n",
    "\n",
    "        output_sequence, _ = tf.nn.dynamic_rnn(\n",
    "            # instance of a RNN core module\n",
    "            cell=dnc_core,\n",
    "            inputs=input_sequence,\n",
    "            time_major=True,\n",
    "            initial_state=initial_state)\n",
    "\n",
    "        return output_sequence\n",
    "    \n",
    "    def train(self, num_training_iterations, report_interval, checkpoint_dir, graph_dir, restore=False,\n",
    "              performance_thresh=0.7, checkpoint_interval=-1, number_queries_answers=2, verbosity=0):\n",
    "        \"\"\"Trains the DNC and periodically reports the loss.\"\"\"\n",
    "        \n",
    "        # placeholders definition (have to be feed during the session\n",
    "        observations_tensor = tf.placeholder(dtype=tf.float32, shape=(None, self._batch_size, 92))\n",
    "        masks_tensor = tf.placeholder(dtype=tf.float32, shape=(None, self._batch_size))\n",
    "        targets_tensor = tf.placeholder(dtype=tf.float32, shape=(None, self._batch_size, 90))\n",
    "        #  compute the output of the RNN\n",
    "        output_logits = self.run_model(observations_tensor, self._model_output)\n",
    "        #  compute the loss with the respect of the target\n",
    "        train_loss = masked_sigmoid_cross_entropy(output_logits, targets_tensor, masks_tensor)\n",
    "        # get all the variables to train\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        #  compute the gradient\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(train_loss, trainable_variables), self._max_grad_norm)\n",
    "\n",
    "        # define a global variable\n",
    "        global_step = tf.get_variable(\n",
    "            name=\"global_step\",\n",
    "            shape=[],\n",
    "            dtype=tf.int64,\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            trainable=False,\n",
    "            collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n",
    "\n",
    "        #  run the backward gradient propagation\n",
    "        optimizer = tf.train.RMSPropOptimizer(\n",
    "            self._learning_rate, epsilon=self._optimizier_epsy)\n",
    "        # GLOBAL -> this is incremented by one after the minimization (backward) have been executed\n",
    "        train_step = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables), global_step=global_step)\n",
    "\n",
    "        # create an instance of Saver to save and restore models\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # remove the model if not wanted to restore it but wanted to save it\n",
    "        if not restore and checkpoint_interval > 0 and os.path.isdir(checkpoint_dir):\n",
    "            tf.logging.info(\" - RESTORE: The old model has been deleted!\")\n",
    "            shutil.rmtree(checkpoint_dir)\n",
    "        # if want to save the new one (no folder found)\n",
    "        if checkpoint_interval > 0:\n",
    "            tf.logging.info(\" - RESTORE: The new model will be saved!\")\n",
    "            hooks = [\n",
    "                tf.train.CheckpointSaverHook(\n",
    "                    checkpoint_dir=checkpoint_dir,\n",
    "                    save_steps=checkpoint_interval,\n",
    "                    saver=saver)\n",
    "            ]\n",
    "        else:\n",
    "            hooks = []\n",
    "\n",
    "        # Monitored Session will automatically restore the model if there is a checkpoint\n",
    "        with tf.train.SingularMonitoredSession(\n",
    "                hooks=hooks, checkpoint_dir=checkpoint_dir) as sess:\n",
    "            # retrieve the global variable value for the iterartion steps\n",
    "            start_iteration = sess.run(global_step)\n",
    "            # ptint information about the previous iteration steps and set the starting interation to 0\n",
    "            if restore:\n",
    "                tf.logging.info(\" - RESTORE: The previous model iteration stopped: {}.\".format(start_iteration))\n",
    "                tf.logging.info(\" - RESTORE: Iteration will restart from 0!\")\n",
    "                start_iteration = 0\n",
    "            tf.logging.info(\" - TRAINING: Training has started!\")\n",
    "            # create the dataset\n",
    "            dataset_traversal = DatasetTraversalGenerator(self._lower_bound, self._upper_bound,\n",
    "                                                          self._nodes, self._walk_length,\n",
    "                                                          self._walk_deepness, graph_dir,\n",
    "                                                          self._batch_size)\n",
    "\n",
    "            # check if it's wanted to save the graph for future training\n",
    "            if restore and os.path.isdir(graph_dir):\n",
    "                tf.logging.info(\" - RESTORE: The graph is loaded!\")\n",
    "                dataset_traversal.load_graph_dataset()\n",
    "            else:\n",
    "                try:\n",
    "                    os.mkdir(graph_dir)\n",
    "                except OSError:\n",
    "                    shutil.rmtree(graph_dir)\n",
    "                    os.mkdir(graph_dir)\n",
    "                tf.logging.info(\" - RESTORE: The graph is saved!\")\n",
    "                dataset_traversal.save_graph_dataset()\n",
    "\n",
    "            total_loss = 0\n",
    "            output_log = None\n",
    "            targets = None\n",
    "            \n",
    "            # result saver\n",
    "            results = {\n",
    "                \"iterations\": [],\n",
    "                \"performances\": [],\n",
    "                \"average_loss\": [],\n",
    "                \"saved_strings\": [],\n",
    "                \"graph\": None,\n",
    "            }\n",
    "            \n",
    "            results[\"graph\"] = dataset_traversal._genGraph.get_graph\n",
    "            \n",
    "            for train_iteration in range(start_iteration, num_training_iterations):\n",
    "                # generate different random walks at each iterations\n",
    "                dataset_traversal._genGraph.gen_random_walks()\n",
    "                # shuffle the observation and generate the walks triple\n",
    "                dataset = dataset_traversal.generate_batches(size=1)\n",
    "                # dataset inputs\n",
    "                observations = dataset['observations']\n",
    "                targets = dataset['targets']\n",
    "                masks = dataset['masks']\n",
    "                # create a feed dictionary for initialize the placeholders\n",
    "                feed = {observations_tensor: observations, masks_tensor: masks, targets_tensor: targets}\n",
    "                _, loss, output_log = sess.run([train_step, train_loss, output_logits], feed_dict=feed)\n",
    "                \n",
    "                \n",
    "                if verbosity != 0:\n",
    "                    tf.logging.info(' - iter:' + str(train_iteration) + ' -- loss: ' + str(loss))\n",
    "                total_loss += loss\n",
    "\n",
    "                if (train_iteration + 1) % report_interval == 0:\n",
    "                    tf.logging.info(' - Average loss: ' + str(total_loss / report_interval))\n",
    "                    performance = evaluate_prediction(output_log, masks, targets)\n",
    "                    tf.logging.info(' - TRAINING: Performances: {} %'.format(performance * 100))\n",
    "                    total_loss = 0\n",
    "                    \n",
    "                    results['iterations'].append(train_iteration)\n",
    "                    results['performances'].append(performance)\n",
    "                    results['average_loss'].append(total_loss / report_interval)\n",
    "\n",
    "                    if performance > performance_thresh:\n",
    "                        # print strings\n",
    "                        queries_answers = decode_predictions_with_queries(output_log, \n",
    "                                                                          observations, \n",
    "                                                                          targets, masks,\n",
    "                                                                          dataset_traversal.query_index_start)\n",
    "\n",
    "                        strings = query_answer_human_readable(queries_answers, number_queries_answers)\n",
    "                        results[\"saved_strings\"].append(strings) \n",
    "                        for string in strings:\n",
    "                            tf.logging.info(\" - RESULTS: \\n\" + string)\n",
    "                    else:\n",
    "                        results[\"saved_strings\"].append(None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model \n",
    "The following class allows to use a trained model to compute prediction given an observation tensor. The tensor should include the graph descritpion phase, the query and answers. In addition, the *evaluate_observations* methods need the mask and the targets to print the obtained results along with the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(object):\n",
    "    \"\"\"Class used to test the model once has been trained.\"\"\"\n",
    "    def __init__(self, hidden_size, memory_size, word_size, num_write_heads, num_read_heads, \n",
    "                 clip_value, model_output, batch_size):\n",
    "        \n",
    "        # model parameters\n",
    "        self._hidden_size = hidden_size \n",
    "        self._memory_size = memory_size\n",
    "        self._word_size = word_size\n",
    "        self._num_write_heads = num_write_heads\n",
    "        self._num_read_heads = num_read_heads\n",
    "        self._clip_value = clip_value\n",
    "        self._model_output = model_output\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "    \n",
    "    def run_model(self, input_sequence, output_size):\n",
    "        \"\"\"Runs model on input sequence.\"\"\"\n",
    "\n",
    "        access_config = {\n",
    "            \"memory_size\": self._memory_size,\n",
    "            \"word_size\": self._word_size,\n",
    "            \"num_reads\": self._num_read_heads,\n",
    "            \"num_writes\": self._num_write_heads,\n",
    "        }\n",
    "        controller_config = {\n",
    "            \"hidden_size\": self._hidden_size,\n",
    "        }\n",
    "        clip_value = self._clip_value\n",
    "        \n",
    "        dnc_core = _dnc.DNC(access_config, controller_config, output_size, clip_value)\n",
    "        initial_state = dnc_core.initial_state(self._batch_size)\n",
    "\n",
    "        output_sequence, _ = tf.nn.dynamic_rnn(\n",
    "            # instance of a RNN core module\n",
    "            cell=dnc_core,\n",
    "            inputs=input_sequence,\n",
    "            time_major=True,\n",
    "            initial_state=initial_state)\n",
    "\n",
    "        return output_sequence\n",
    "    \n",
    "    \n",
    "    def evaluate(self, obs, masks, targets, query_index_start, checkpoint_dir):\n",
    "        \"\"\"TODO\"\"\"\n",
    "        model_file = tf.train.latest_checkpoint(checkpoint_dir) + \".meta\"\n",
    "        # recover the graph from the previous saved model\n",
    "        saver = tf.train.import_meta_graph(model_file)\n",
    "        tf.logging.info(\"Restoring model from {}\".format(model_file))\n",
    "\n",
    "        observations_tensor = tf.placeholder(dtype=tf.float32, shape=(None, self._batch_size, 92))\n",
    "        output_logits = self.run_model(observations_tensor, self._model_output)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            # initialize the variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # lead the saved variables\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "            # feed the paceholders\n",
    "            feed = {observations_tensor: obs}\n",
    "            output_log = sess.run([output_logits], feed_dict=feed)\n",
    "            outputs = np.array(output_log[0])\n",
    "        \n",
    "        performance = evaluate_prediction(outputs, masks, targets)\n",
    "        tf.logging.info(' - TRAINING: Performances: {} %'.format(performance * 100))\n",
    "\n",
    "        queries_answers = decode_predictions_with_queries(outputs, observations, targets, masks,\n",
    "                                                             query_index_start)\n",
    "\n",
    "        strings = query_answer_human_readable(queries_answers)\n",
    "\n",
    "        for string in strings:\n",
    "            tf.logging.info(\" - PREDICTIONS: \\n\" + string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "Now we test the DNC on a small graph task. To notice, there is a relation between the number of triples and the memory size (especially the number of entries). To avoid large matric computation we train the model on a given graph with a low number of nodes. This ensure a relative low number of triples which can be easily stored using a memory of 128 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "HIDDEN_SIZE = 64\n",
    "MEMORY_SIZE = 128\n",
    "WORD_SIZE = 16\n",
    "NUM_WRITE_HEADS = 1\n",
    "NUM_READ_HEADS = 4\n",
    "CLIP_VALUE = 20\n",
    "MODEL_OUTPU_SIZE = 90\n",
    "\n",
    "# Optimizer parameters.\n",
    "MAX_GRAD_NORM = 50\n",
    "LEARN_RATE = 1e-4\n",
    "OPTIMIZER_EPSY = 1e-10\n",
    "\n",
    "# Task parameters\n",
    "BATCH_SIZE = 16\n",
    "LOWER_BOUND =  2\n",
    "UPPER_BOUND = 4\n",
    "NODES = 20\n",
    "WALK_LENGTH = 2\n",
    "WALK_DEEPNESS = 2\n",
    "\n",
    "# Training options.\n",
    "TRAINING_ITER = 10\n",
    "REPORT_INTERV = 10\n",
    "CHECHK_POINT_DIR = \"./tf/dnc/traversal\"\n",
    "GRAPH_SAVE_DIR = \"./nx/dnc/traversal\"\n",
    "CHECK_POINT_INTERVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: - RESTORE: The new model will be saved!\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tf/dnc/traversal/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./tf/dnc/traversal/model.ckpt.\n",
      "INFO:tensorflow: - RESTORE: The previous model iteration stopped: 0.\n",
      "INFO:tensorflow: - RESTORE: Iteration will restart from 0!\n",
      "INFO:tensorflow: - TRAINING: Training has started!\n",
      "INFO:tensorflow: - RESTORE: The graph is loaded!\n",
      "INFO:tensorflow: - iter:0 -- loss: 5634.054\n",
      "INFO:tensorflow: - iter:1 -- loss: 7133.587\n",
      "INFO:tensorflow: - iter:2 -- loss: 6383.521\n",
      "INFO:tensorflow: - iter:3 -- loss: 6382.1704\n",
      "INFO:tensorflow: - iter:4 -- loss: 5628.5566\n",
      "INFO:tensorflow: - iter:5 -- loss: 6753.417\n",
      "INFO:tensorflow: - iter:6 -- loss: 5622.791\n",
      "INFO:tensorflow: - iter:7 -- loss: 5997.0557\n",
      "INFO:tensorflow: - iter:8 -- loss: 5623.6875\n",
      "INFO:tensorflow: - iter:9 -- loss: 6372.921\n",
      "INFO:tensorflow: - Average loss: 6153.176123046875\n",
      "INFO:tensorflow: - TRAINING: Performances: 0.0 %\n",
      "INFO:tensorflow:Saving checkpoints for 10 into ./tf/dnc/traversal/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "training = TrainingTraversal(HIDDEN_SIZE, MEMORY_SIZE, WORD_SIZE, NUM_WRITE_HEADS, NUM_READ_HEADS, \n",
    "                   CLIP_VALUE, MODEL_OUTPU_SIZE, MAX_GRAD_NORM, LEARN_RATE, OPTIMIZER_EPSY, BATCH_SIZE, \n",
    "                   LOWER_BOUND, UPPER_BOUND, NODES, WALK_LENGTH, WALK_DEEPNESS)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "training.train(TRAINING_ITER, REPORT_INTERV, CHECHK_POINT_DIR, GRAPH_SAVE_DIR, performance_thresh=0.7, \n",
    "              checkpoint_interval=100, number_queries_answers=2, restore=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the trained model\n",
    "Once trained, the model can be used to make predictions on a given observation. It needs also a mask and a target.\n",
    "The model is loaded from a given folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 64\n",
    "MEMORY_SIZE = 128\n",
    "WORD_SIZE = 16\n",
    "NUM_WRITE_HEADS = 1\n",
    "NUM_READ_HEADS = 4\n",
    "CLIP_VALUE = 20\n",
    "MODEL_OUTPU_SIZE = 90\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_traversal = DatasetTraversalGenerator(LOWER_BOUND, UPPER_BOUND, NODES, WALK_LENGTH, WALK_DEEPNESS, \n",
    "                                              GRAPH_SAVE_DIR, BATCH_SIZE)\n",
    "\n",
    "# generate different random walks\n",
    "dataset_traversal._genGraph.gen_random_walks()\n",
    "# shuffle the observation and generate the walks triple\n",
    "dataset = dataset_traversal.generate_batches(size=1)\n",
    "\n",
    "observations = dataset['observations']\n",
    "targets = dataset['targets']\n",
    "masks = dataset['masks']\n",
    "index_start = dataset_traversal.query_index_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring model from ./tf/dnc/traversal/model.ckpt-10.meta\n",
      "INFO:tensorflow:Restoring parameters from ./tf/dnc/traversal/model.ckpt-10\n",
      "INFO:tensorflow: - TRAINING: Performances: 0.0 %\n",
      "INFO:tensorflow: - PREDICTIONS: \n",
      "#### QA 0 ####\n",
      ">>> Queries:\n",
      "(373, 836, _)\n",
      "(_, 791, _)\n",
      "(_, 612, _)\n",
      "(_, 650, _)\n",
      "(_, 612, _)\n",
      ">>> Answers:\n",
      "(731, 917, 280)\n",
      "(131, 917, 280)\n",
      "(131, 917, 480)\n",
      "(131, 917, 480)\n",
      "(131, 911, 480)\n",
      ">>> Targets:\n",
      "(373, 836, 853)\n",
      "(853, 791, 612)\n",
      "(612, 612, 184)\n",
      "(184, 650, 843)\n",
      "(843, 612, 370)\n",
      "\n",
      "\n",
      "INFO:tensorflow: - PREDICTIONS: \n",
      "#### QA 1 ####\n",
      ">>> Queries:\n",
      "(840, 836, _)\n",
      "(_, 239, _)\n",
      "(_, 628, _)\n",
      "(_, 409, _)\n",
      "(_, 843, _)\n",
      ">>> Answers:\n",
      "(131, 112, 580)\n",
      "(131, 112, 580)\n",
      "(131, 114, 580)\n",
      "(131, 114, 480)\n",
      "(131, 114, 480)\n",
      ">>> Targets:\n",
      "(840, 836, 628)\n",
      "(628, 239, 178)\n",
      "(178, 628, 836)\n",
      "(836, 409, 502)\n",
      "(502, 843, 409)\n",
      "\n",
      "\n",
      "INFO:tensorflow: - PREDICTIONS: \n",
      "#### QA 2 ####\n",
      ">>> Queries:\n",
      "(628, 836, _)\n",
      "(_, 913, _)\n",
      "(_, 184, _)\n",
      "(_, 843, _)\n",
      "(_, 409, _)\n",
      ">>> Answers:\n",
      "(131, 912, 580)\n",
      "(131, 912, 580)\n",
      "(131, 912, 580)\n",
      "(131, 914, 580)\n",
      "(131, 914, 580)\n",
      ">>> Targets:\n",
      "(628, 836, 840)\n",
      "(840, 913, 178)\n",
      "(178, 184, 409)\n",
      "(409, 843, 502)\n",
      "(502, 409, 836)\n",
      "\n",
      "\n",
      "INFO:tensorflow: - PREDICTIONS: \n",
      "#### QA 3 ####\n",
      ">>> Queries:\n",
      "(370, 612, _)\n",
      "(_, 650, _)\n",
      "(_, 628, _)\n",
      "(_, 836, _)\n",
      "(_, 791, _)\n",
      ">>> Answers:\n",
      "(121, 712, 580)\n",
      "(121, 712, 480)\n",
      "(121, 714, 480)\n",
      "(131, 714, 480)\n",
      "(131, 714, 580)\n",
      ">>> Targets:\n",
      "(370, 612, 843)\n",
      "(843, 650, 184)\n",
      "(184, 628, 373)\n",
      "(373, 836, 853)\n",
      "(853, 791, 612)\n",
      "\n",
      "\n",
      "INFO:tensorflow: - PREDICTIONS: \n",
      "#### QA 4 ####\n",
      ">>> Queries:\n",
      "(409, 612, _)\n",
      "(_, 628, _)\n",
      "(_, 931, _)\n",
      "(_, 525, _)\n",
      "(_, 836, _)\n",
      ">>> Answers:\n",
      "(131, 912, 580)\n",
      "(131, 914, 580)\n",
      "(131, 914, 580)\n",
      "(141, 914, 580)\n",
      "(141, 914, 580)\n",
      ">>> Targets:\n",
      "(409, 612, 836)\n",
      "(836, 628, 178)\n",
      "(178, 931, 502)\n",
      "(502, 525, 840)\n",
      "(840, 836, 628)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model = TestModel(HIDDEN_SIZE, MEMORY_SIZE, WORD_SIZE, NUM_WRITE_HEADS, NUM_READ_HEADS, \n",
    "                       CLIP_VALUE, MODEL_OUTPU_SIZE, BATCH_SIZE)\n",
    "test_model.evaluate(observations, masks, targets, index_start, CHECHK_POINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
