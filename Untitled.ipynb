{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Neural Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 forms of attention for heads (a way to frame how importance or weighting is important when reading and writing)\n",
    "\n",
    "1. content lookup \n",
    "    The key tells whihc value you have to recover from the memory. Find the cosine similarity to address the memory.\n",
    "2. records transitions between consecutively written locations in an N × N temporal link matrix L. This gives a DNC the native ability to recover sequences in the order in which it wrote them, even when consecutive writes did not occur in adjacent time-step.\n",
    "3. The third form of attention allocates memory for writing.\n",
    "    This is a dynamic allocation for the memory, allocating and erasing dynamically, intead of being static.\n",
    "    \n",
    "<br>\n",
    "Content lookup enables the formation of associative data structures; temporal links enable sequential retrieval of input sequences; and allocation provides the write head with unused locations.\n",
    "DNC memory modification is fast and can be one-shot, resembling the associative long-term potentiation of hippocampal CA3 and CA1 synapses\n",
    "Human ‘free recall’ experiments demonstrate the increased probability of item recall in the same order as first pre-sented (temporal links)\n",
    "DeepMind hopes that DNCs provide both a new tool for computer science and a new metaphor for cognitive science and neuroscience: here is a learning machine that, without prior programming, can organise information into connected facts and use those facts to solve problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC:\n",
    "    def __init__(self, input_size, output_size, seq_len, num_words=256, word_size=64, num_heads=4):\n",
    "        #define data\n",
    "        #input data - [[1 0] [0 1] [0 0] [0 0]]\n",
    "        # it will be a (4,2) ? \n",
    "        self.input_size = input_size #X\n",
    "        #output data [[0 0] [0 0] [1 0] [0 1]]\n",
    "        self.output_size = output_size #Y\n",
    "        \n",
    "        # MATRIX MEMORY DEF\n",
    "        \n",
    "        #define read + write vector size\n",
    "        # this will define also the memory matrix\n",
    "        #10\n",
    "        self.num_words = num_words #N\n",
    "        #4 characters\n",
    "        self.word_size = word_size #W\n",
    "        \n",
    "        # READ AND WRITE HEADS \n",
    "        \n",
    "        #define number of read+write heads\n",
    "        #we could have multiple, but just 1 for simplicity\n",
    "        # see it has an operation\n",
    "        # it is how many times we read and write form the memort for each feedforward (time step?)\n",
    "        self.num_heads = num_heads #R\n",
    "        \n",
    "        # INTERFACE VECTOR\n",
    "        \n",
    "        #  the interface vector is the other output in addition to the normal output of the network\n",
    "        # it defines how the controller will interact with the memory bank at the next time step\n",
    "        #size of output vector from controller that defines interactions with memory matrix\n",
    "        self.interface_size = (num_heads * word_size) + (3 * word_size) + (5 * num_heads) + 3\n",
    "        # those magic numbers are hyperparams\n",
    "        \n",
    "        \n",
    "        # SIZE OF THE NN\n",
    "        \n",
    "        #the actual size of the neural network input after flatenning and\n",
    "        #concatenating the input vector with the previously read vctors from memory (word_size)\n",
    "        self.nn_input_size = ( num_heads * word_size ) + input_size\n",
    "        #size of output(it will include also the vector of the interface)\n",
    "        self.nn_output_size = output_size + self.interface_size\n",
    "        \n",
    "        # why???\n",
    "        #gaussian normal distribution for both outputs\n",
    "        self.nn_out = tf.truncated_normal([1, self.output_size], stddev=0.1)\n",
    "        self.interface_vec = tf.truncated_normal([1, self.interface_size], stddev=0.1)\n",
    "        \n",
    "        # MEMORY MATRIX FOR WORDS\n",
    "        \n",
    "        #Create memory matrix\n",
    "        self.mem_mat = tf.zeros([num_words, word_size]) #N*W\n",
    "        \n",
    "        \n",
    "        # THE ASIDE MATRIX FOR SUPPORT READ AND WRITE OP\n",
    "        \n",
    "        #other variables (those concerning the other matrix: that is the one which takes into account the\n",
    "        # sequence of writings)\n",
    "        #The usage vector records which locations have been used so far, \n",
    "        self.usage_vec = tf.fill([num_words, 1], 1e-6) #N*1\n",
    "        #a temporal link matrix records the order in which locations were written; \n",
    "        # (defined using the usage_vector)\n",
    "        self.link_mat = tf.zeros([num_words,num_words]) #N*N\n",
    "        #represents degrees to which last location was written to\n",
    "        self.precedence_weight = tf.zeros([num_words, 1]) #N*1\n",
    "\n",
    "        \n",
    "        # WEIGHTS FOR THE WRITE AND READ HEADS\n",
    "        \n",
    "        # remind we have one head but we have weights for that head. They are used to define the degree of\n",
    "        # reading and writing. Read and write are just matrix moltiplication, we can tune how much we are doing\n",
    "        # this. Similar to a learning rate. Also those weights are differentiated\n",
    "        #Read and write head weight variables\n",
    "        self.read_weights = tf.fill([num_words, num_heads], 1e-6) #N*R\n",
    "        self.write_weights = tf.fill([num_words, 1], 1e-6) #N*1\n",
    "        self.read_vecs = tf.fill([num_heads, word_size], 1e-6) #R*W\n",
    "\n",
    "        ###NETWORK VARIABLES###\n",
    "        \n",
    "        #PLACEHOLDERS\n",
    "        \n",
    "        #gateways into the computation graph for input output pairs\n",
    "        # it's like initializing the inputs and outputs\n",
    "        self.i_data = tf.placeholder(tf.float32, [seq_len*2, self.input_size], name='input_node')\n",
    "        self.o_data = tf.placeholder(tf.float32, [seq_len*2, self.output_size], name='output_node')\n",
    "        \n",
    "        # NETWORK LAYER WEIGHTS AND BIAS\n",
    "        \n",
    "        #2 layer feedforwarded network\n",
    "        # NOTE: it's using 32 neurons for the hidden layers\n",
    "        self.W1 = tf.Variable(tf.truncated_normal([self.nn_input_size, 32], stddev=0.1), \\ \n",
    "                              name='layer1_weights', dtype=tf.float32)\n",
    "        self.b1 = tf.Variable(tf.zeros([32]), \\\n",
    "                              name='layer1_bias', dtype=tf.float32)\n",
    "        self.W2 = tf.Variable(tf.truncated_normal([32, self.nn_output_size], stddev=0.1), \\\n",
    "                              name='layer2_weights', dtype=tf.float32)\n",
    "        self.b2 = tf.Variable(tf.zeros([self.nn_output_size]), \\\n",
    "                              name='layer2_bias', dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        ###DNC OUTPUT WEIGHTS\n",
    "        \n",
    "        # we have also weights for the output due to everything can be differentiated\n",
    "        # NOTE: those are the weights form the second hidden layer to the output\n",
    "        self.nn_out_weights = tf.Variable(tf.truncated_normal([self.nn_output_size, self.output_size], stddev=0.1), name='net_output_weights')\n",
    "        self.interface_weights = tf.Variable(tf.truncated_normal([self.nn_output_size, self.interface_size], stddev=0.1), name='interface_weights')\n",
    "        \n",
    "        # this for sure is related with the memory\n",
    "        self.read_vecs_out_weight = tf.Variable(tf.truncated_normal([self.num_heads*self.word_size, self.output_size], stddev=0.1), name='read_vector_weights')\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    #################################################    \n",
    "    #3 attention mechanisms for read/writes to memory\n",
    "    #################################################\n",
    "    \n",
    "    #### FIRST DIFFERENTIABLE ATTENTION ####\n",
    "    \n",
    "    #a key vector emitted by the controller is compared to the \n",
    "    #content of each location in memory according to a -similarity measure- \n",
    "    #The similarity scores determine a weighting that can be used by the read heads \n",
    "    #for associative recall1 or by the write head to modify an existing vector in memory.\n",
    "    def content_lookup(self, key, string):\n",
    "        # The l2 norm of a vector is the square root of the sum of the \n",
    "        # absolute values squared\n",
    "        # applied to the memory\n",
    "        norm_mem = tf.nn.l2_normalize(self.mem_mat, 1) #N*W\n",
    "        # applied to the key (emitted from the NN)\n",
    "        norm_key = tf.nn.l2_normalize(key, 0) #1*W for write or R*W for read\n",
    "        \n",
    "        #get similarity measure between both vectors, transpose before multiplicaiton\n",
    "        ##(N*W,W*1)->N*1 for write\n",
    "        #(N*W,W*R)->N*R for read\n",
    "        sim = tf.matmul(norm_mem, norm_key, transpose_b=True) \n",
    "        #str is 1*1 or 1*R\n",
    "        \n",
    "        #returns similarity measure\n",
    "        return tf.nn.softmax(sim*string, 0) #N*1 or N*R\n",
    "\n",
    "    \n",
    "     #### SECOND DIFFERENTIABLE ATTENTION ####\n",
    "    \n",
    "    #retreives the writing allocation weighting based on the usage free list\n",
    "    #The ‘usage’ of each location is represented as a number between 0 and 1, \n",
    "    #and a weighting that picks out unused locations is delivered to the write head. \n",
    "    \n",
    "    # independent of the size and contents of the memory, meaning that \n",
    "    #DNCs can be trained to solve a task using one size of memory and later \n",
    "    \n",
    "    # NOTE: upgraded to a larger memory without retraining\n",
    "    def allocation_weighting(self):\n",
    "        #sorted usage - the usage vector sorted ascendingly\n",
    "        #the original indices of the sorted usage vector\n",
    "        sorted_usage_vec, free_list = tf.nn.top_k(-1 * self.usage_vec, k=self.num_words)\n",
    "        sorted_usage_vec *= -1\n",
    "        # cumulative product\n",
    "        cumprod = tf.cumprod(sorted_usage_vec, axis=0, exclusive=True)\n",
    "        unorder = (1-sorted_usage_vec)*cumprod\n",
    "\n",
    "        alloc_weights = tf.zeros([self.num_words])\n",
    "        I = tf.constant(np.identity(self.num_words, dtype=np.float32))\n",
    "        \n",
    "        #for each usage vec\n",
    "        for pos, idx in enumerate(tf.unstack(free_list[0])):\n",
    "            #flatten\n",
    "            m = tf.squeeze(tf.slice(I, [idx, 0], [1, -1]))\n",
    "            #add to weight matrix\n",
    "            alloc_weights += m*unorder[0, pos]\n",
    "        #the allocation weighting for each row in memory\n",
    "        # define how best we want to allocate memory, so it's a matrix\n",
    "        return tf.reshape(alloc_weights, [self.num_words, 1])\n",
    "\n",
    "    \n",
    "    # >>>>> STEP DEFINITION <<<<< #\n",
    "    \n",
    "    #at every time step the controller receives input vector from dataset and emits output vector. \n",
    "    #it also recieves a set of read vectors from the memory matrix at the previous time step via \n",
    "    #the read heads. then it emits an interface vector that defines its interactions with the memory\n",
    "    #at the current time step\n",
    "    def step_m(self, x):\n",
    "        \n",
    "        ### INPUT RESHAPE\n",
    "        \n",
    "        # the input is not just data, it's also the read from the memory at the previous step. \n",
    "        # the NN is a Feedforward one but in general is active recursevly due to the memory.\n",
    "        # We are not feeding the previous state of the network, but previous red data\n",
    "        input = tf.concat(\n",
    "            [\n",
    "                x, # the dataset one\n",
    "                tf.reshape(\n",
    "                    self.read_vecs, # we concatenate the previous read\n",
    "                    [1, (self.num_heads * self.word_size)] # adapted to the size (1, heads*words)\n",
    "                )\n",
    "            ],\n",
    "            1 # axis\n",
    "        )\n",
    "        \n",
    "        ### FORWARD PASS\n",
    "        \n",
    "        #forward propagation\n",
    "        # activation layer 1h\n",
    "        l1_out = tf.matmul(input, self.W1) + self.b1\n",
    "        # output layer 1h\n",
    "        l1_act = tf.nn.tanh(l1_out)\n",
    "        # activation layer 2h\n",
    "        l2_out = tf.matmul(l1_act, self.W2) + self.b2\n",
    "        # output layer 2h\n",
    "        l2_act = tf.nn.tanh(l2_out)\n",
    "        \n",
    "        #output vector\n",
    "        self.nn_out = tf.matmul(l2_act, self.nn_out_weights) #(1*eta+Y, eta+Y*Y)->(1*Y)\n",
    "        \n",
    "        #interaction vector - how to interact with memory\n",
    "        self.interface_vec = tf.matmul(l2_act, self.interface_weights) #(1*eta+Y, eta+Y*eta)->(1*eta)\n",
    "        \n",
    "        \n",
    "        ### MEMORY MATRIX INTERACTION\n",
    "        \n",
    "        # the idea is to split the interface vector according to some strategy (take a big array and split \n",
    "        # it's more like dealing with PCB design, where encoding bits and concatenating them)\n",
    "        partition = tf.constant([[0]*(self.num_heads*self.word_size) + [1]*(self.num_heads) + [2]*(self.word_size) + [3] + \\\n",
    "                    [4]*(self.word_size) + [5]*(self.word_size) + \\\n",
    "                    [6]*(self.num_heads) + [7] + [8] + [9]*(self.num_heads*3)], dtype=tf.int32)\n",
    "\n",
    "        #convert interface vector into a set of read write vectors\n",
    "        #using tf.dynamic_partitions(Partitions interface_vec into 10 tensors using indices from partition)\n",
    "        (\n",
    "            read_keys, \n",
    "            read_str, \n",
    "            write_key, \n",
    "            write_str,\n",
    "            erase_vec, \n",
    "            write_vec, \n",
    "            free_gates, \n",
    "            alloc_gate, \n",
    "            write_gate, \n",
    "            read_modes\n",
    "        ) = \\\n",
    "            # see what this dynamic partition is\n",
    "            tf.dynamic_partition(\n",
    "                self.interface_vec, \n",
    "                partition, \n",
    "                10\n",
    "            )\n",
    "        \n",
    "        # MEMORY\n",
    "        \n",
    "        #read vectors (content address)\n",
    "        read_keys = tf.reshape(read_keys,[self.num_heads, self.word_size]) #R*W\n",
    "        # helps to initialize the read\n",
    "        read_str = 1 + \\\n",
    "            # what does it do ?\n",
    "            tf.nn.softplus(\n",
    "                tf.expand_dims(read_str, 0)\n",
    "            ) #1*R\n",
    "        \n",
    "        #write vectors (content address)\n",
    "        write_key = tf.expand_dims(write_key, 0) #1*W\n",
    "        # helps to initialize the write\n",
    "        write_str = 1 + \\\n",
    "            # what does it do ?\n",
    "            tf.nn.softplus(\n",
    "                tf.expand_dims(write_str, 0)\n",
    "            ) #1*1\n",
    "        # overwrite \n",
    "        erase_vec = tf.nn.sigmoid(tf.expand_dims(erase_vec, 0)) #1*W\n",
    "        write_vec = tf.expand_dims(write_vec, 0) #1*W\n",
    "        \n",
    "        # GATES INITIALIZATIONS \n",
    "        \n",
    "        # gate in the sense of define the degree of the operations (like LSTM or GRU networks)\n",
    "        # this gates value, which are weights, are differentiables\n",
    "        \n",
    "        #the degree to which locations at read heads will be freed\n",
    "        free_gates = tf.nn.sigmoid(tf.expand_dims(free_gates, 0)) #1*R\n",
    "        \n",
    "        #the fraction of writing that is being allocated in a new location\n",
    "        # dynamic memory allocation (not static)\n",
    "        alloc_gate = tf.nn.sigmoid(alloc_gate) #1\n",
    "        \n",
    "        #the amount of information to be written to memory\n",
    "        write_gate = tf.nn.sigmoid(write_gate) #1\n",
    "        \n",
    "        # READ MODES \n",
    "        \n",
    "        #the softmax distribution between the three read modes (backward, forward, lookup)\n",
    "        #The read heads can use gates called read modes to switch between content lookup \n",
    "        #using a read key and reading out locations either forwards or backwards \n",
    "        #in the order they were written.\n",
    "        read_modes = tf.nn.softmax(tf.reshape(read_modes, [3, self.num_heads])) #3*R\n",
    "        \n",
    "        ### WRITE FIRST\n",
    "        \n",
    "        # ALLOCATION DECISION\n",
    "        \n",
    "        #used to calculate usage vector, what's available to write to?\n",
    "        retention_vec = tf.reduce_prod(1-free_gates*self.read_weights, reduction_indices=1)\n",
    "        #used to dynamically allocate memory\n",
    "        self.usage_vec = (self.usage_vec + self.write_weights - self.usage_vec * self.write_weights) * retention_vec\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##retreives the writing allocation weighting \n",
    "        alloc_weights = self.allocation_weighting() #N*1\n",
    "        #where to write to??\n",
    "        write_lookup_weights = self.content_lookup(write_key, write_str) #N*1\n",
    "        #define our write weights now that we know how much space to allocate for them and where to write to\n",
    "        self.write_weights = write_gate*(alloc_gate*alloc_weights + (1-alloc_gate)*write_lookup_weights)\n",
    "\n",
    "        #write erase, then write to memory!\n",
    "        self.mem_mat = self.mem_mat*(1-tf.matmul(self.write_weights, erase_vec)) + \\\n",
    "                       tf.matmul(self.write_weights, write_vec)\n",
    "\n",
    "            \n",
    "        ### READING THEN    \n",
    "        \n",
    "        #As well as writing, the controller can read from multiple locations in memory. \n",
    "        #Memory can be searched based on the content of each location, or the associative \n",
    "        #temporal links can be followed forward and backward to recall information written \n",
    "        #in sequence or in reverse. (3rd attention mechanism)\n",
    "        \n",
    "        #updates and returns the temporal link matrix for the latest write\n",
    "        #given the precedence vector and the link matrix from previous step\n",
    "        nnweight_vec = tf.matmul(self.write_weights, tf.ones([1,self.num_words])) #N*N\n",
    "        self.link_mat = (1 - nnweight_vec - tf.transpose(nnweight_vec))*self.link_mat + \\\n",
    "                        tf.matmul(self.write_weights, self.precedence_weight, transpose_b=True)\n",
    "        self.link_mat *= tf.ones([self.num_words, self.num_words]) - tf.constant(np.identity(self.num_words, dtype=np.float32))\n",
    "\n",
    "        # update of the precedence weights\n",
    "        self.precedence_weight = (1-tf.reduce_sum(self.write_weights, reduction_indices=0)) * \\\n",
    "                                 self.precedence_weight + self.write_weights\n",
    "            \n",
    "         #### THIRD DIFFERENTIABLE ATTENTION #### \n",
    "        \n",
    "        #3 modes - forward, backward, content lookup\n",
    "        forw_w = read_modes[2]*tf.matmul(self.link_mat, self.read_weights) #(N*N,N*R)->N*R\n",
    "        look_w = read_modes[1]*self.content_lookup(read_keys, read_str) #N*R\n",
    "        back_w = read_modes[0]*tf.matmul(self.link_mat, self.read_weights, transpose_a=True) #N*R\n",
    "        \n",
    "        \n",
    "        # use them to intiialize read weights\n",
    "        self.read_weights = back_w + look_w + forw_w #N*R\n",
    "        \n",
    "        #create read vectors by applying read weights to memory matrix\n",
    "        self.read_vecs = tf.transpose(tf.matmul(self.mem_mat, self.read_weights, transpose_a=True)) #(W*N,N*R)^T->R*W\n",
    "        \n",
    "        # FINAL READ\n",
    "        \n",
    "        #multiply them together\n",
    "        read_vec_mut = tf.matmul(tf.reshape(self.read_vecs, [1, self.num_heads * self.word_size]),\n",
    "                                 self.read_vecs_out_weight)  # (1*RW, RW*Y)-> (1*Y)\n",
    "        \n",
    "        ### NN OUTPUT (ALL)\n",
    "        \n",
    "        #return output + read vecs product\n",
    "        return self.nn_out+read_vec_mut\n",
    "\n",
    "    #output list of numbers (one hot encoded) by running the step function\n",
    "    def run(self):\n",
    "        big_out = []\n",
    "        for t, seq in enumerate(tf.unstack(self.i_data, axis=0)):\n",
    "            seq = tf.expand_dims(seq, 0)\n",
    "            y = self.step_m(seq)\n",
    "            big_out.append(y)\n",
    "        return tf.stack(big_out, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
